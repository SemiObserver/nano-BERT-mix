{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:21:56.394657Z",
     "start_time": "2023-10-17T21:21:52.683897Z"
    },
    "id": "EzQpePJ4cXPG"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nano_bert.model_ import BertMix3\n",
    "from nano_bert.tokenizer import WordTokenizer\n",
    "torch.manual_seed(114514)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open('data/imdb_train.json') as f:\n",
    "    data = [json.loads(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = None\n",
    "with open('data/imdb_test.json') as f:\n",
    "    test_data = [json.loads(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25000/25000 [00:00<00:00, 66957.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 25000/25000 [00:19<00:00, 1253.97it/s]\n"
     ]
    }
   ],
   "source": [
    "rawvocab = [] # whole vocab\n",
    "for d in tqdm(data): \n",
    "    rawvocab.append([w.lower() for w in d['text']]) # symbol like '.' is remained\n",
    "vocab = set() # vocab for words appear more than 2 times(minappear = 2)\n",
    "minappear = 2\n",
    "for v in tqdm(rawvocab):\n",
    "    if rawvocab.count(v) > minappear - 1:\n",
    "        vocab |= set(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:22:01.652524Z",
     "start_time": "2023-10-17T21:22:01.646787Z"
    },
    "id": "V1EiqscxcXPI"
   },
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "    if label == 'pos':\n",
    "        return 1\n",
    "    elif label == 'neg':\n",
    "        return 0\n",
    "    raise Exception(f'Unknown Label: {label}!')\n",
    "\n",
    "\n",
    "class IMDBDataloader:\n",
    "    def __init__(self, data, test_data, tokenizer, label_encoder, batch_size, val_frac=0.2):\n",
    "        train_data, val_data = train_test_split(data, shuffle=True, random_state=1, test_size=val_frac)\n",
    "\n",
    "        self.splits = {\n",
    "            'train': [d['text'] for d in train_data],\n",
    "            'test': [d['text'] for d in test_data],\n",
    "            'val': [d['text'] for d in val_data]\n",
    "        }\n",
    "\n",
    "        self.labels = {\n",
    "            'train': [d['label'] for d in train_data],\n",
    "            'test': [d['label'] for d in test_data],\n",
    "            'val': [d['label'] for d in val_data]\n",
    "        }\n",
    "\n",
    "        self.tokenized = {\n",
    "            'train': [tokenizer(record).unsqueeze(0) for record in\n",
    "                      tqdm(self.splits['train'], desc='Train Tokenization',position=0)], # divide different sentences in comments\n",
    "            'test': [tokenizer(record).unsqueeze(0) for record in tqdm(self.splits['test'], desc='Test Tokenization',position=0)],\n",
    "            'val': [tokenizer(record).unsqueeze(0) for record in tqdm(self.splits['val'], desc='Val Tokenization',position=0)],\n",
    "        }\n",
    "\n",
    "        self.encoded_labels = {\n",
    "            'train': [label_encoder(label) for label in tqdm(self.labels['train'], desc='Train Label Encoding',position=0)],\n",
    "            'test': [label_encoder(label) for label in tqdm(self.labels['test'], desc='Test Label Encoding',position=0)],\n",
    "            'val': [label_encoder(label) for label in tqdm(self.labels['val'], desc='Val Label Encoding',position=0)],\n",
    "        }\n",
    "\n",
    "        self.curr_batch = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.iterate_split = None\n",
    "\n",
    "    def peek(self, split):\n",
    "        return {\n",
    "            'input_ids': self.splits[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "            'label_ids': self.labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "        }\n",
    "\n",
    "    def take(self, split):\n",
    "        batch = self.splits[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        labels = self.labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        self.curr_batch += 1\n",
    "        return {\n",
    "            'input_ids': batch,\n",
    "            'label_ids': labels,\n",
    "        }\n",
    "\n",
    "    def peek_tokenized(self, split):\n",
    "        return {\n",
    "            'input_ids': torch.cat(\n",
    "                self.tokenized[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "                dim=0),\n",
    "            'label_ids': torch.tensor(\n",
    "                self.encoded_labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "                dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def peek_index_tokenized(self, index, split):\n",
    "        return {\n",
    "            'input_ids': torch.cat(\n",
    "                [self.tokenized[split][index]],\n",
    "                dim=0),\n",
    "            'label_ids': torch.tensor(\n",
    "                [self.encoded_labels[split][index]],\n",
    "                dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def peek_index(self, index, split):\n",
    "        return {\n",
    "            'input_ids': [self.splits[split][index]],\n",
    "            'label_ids': [self.labels[split][index]],\n",
    "        }\n",
    "\n",
    "    def take_tokenized(self, split):\n",
    "        batch = self.tokenized[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        labels = self.encoded_labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        self.curr_batch += 1\n",
    "        return {\n",
    "            'input_ids': torch.cat(batch, dim=0),\n",
    "            'label_ids': torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def get_split(self, split):\n",
    "        self.iterate_split = split\n",
    "        return self\n",
    "\n",
    "    def steps(self, split):\n",
    "        return len(self.tokenized[split]) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.reset()\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.batch_size * self.curr_batch < len(self.splits[self.iterate_split]):\n",
    "            return self.take_tokenized(self.iterate_split)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:22:18.371680Z",
     "start_time": "2023-10-17T21:22:18.368560Z"
    },
    "id": "5b6HjLMEcXPJ"
   },
   "outputs": [],
   "source": [
    "NUM_CLASS = 2\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:23.407128Z",
     "start_time": "2023-10-17T21:26:23.403439Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CdJl4UEcXPJ",
    "outputId": "4ea3a032-0c97-4849-c825-28f189f7f643"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer[vocab=3665,self.special_tokens=['[MSK]', '[PAD]', '[CLS]', '[SEP]', '[UNK]', '[SOS]', '.'],self.sep=' ',self.max_seq_len=128]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.discard('.') # '.' is included in taken's vocab enumerate\n",
    "tokenizer = WordTokenizer(vocab=vocab, max_seq_len=MAX_SEQ_LEN)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:37.278112Z",
     "start_time": "2023-10-17T21:26:24.941273Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQMDcK01cXPK",
    "outputId": "b6b3b189-e1f3-46fd-d819-8ded067160ac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Tokenization: 100%|██████████████████████████████████████████████████████| 20000/20000 [00:05<00:00, 3944.17it/s]\n",
      "Test Tokenization: 100%|███████████████████████████████████████████████████████| 25000/25000 [00:04<00:00, 5077.59it/s]\n",
      "Val Tokenization: 100%|██████████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 5070.44it/s]\n",
      "Train Label Encoding: 100%|█████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 1067483.81it/s]\n",
      "Test Label Encoding: 100%|███████████████████████████████████████████████████| 25000/25000 [00:00<00:00, 991111.36it/s]\n",
      "Val Label Encoding: 100%|██████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 701107.25it/s]\n"
     ]
    }
   ],
   "source": [
    "dataloader = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "         126, 127]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = position = torch.arange(0, 128).unsqueeze(0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionEmbedding(torch.nn.Module): # for DEBERTA\n",
    "    def __init__(self, n_embed, max_seq_len):\n",
    "        super(AbsolutePositionEmbedding, self).__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, n_embed)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # Get the position IDs from the input IDs\n",
    "        position_ids = torch.arange(0, self.max_seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        # Get the position embeddings\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        return position_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.,   0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "         11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,\n",
       "         23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,\n",
       "         35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,\n",
       "         47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,\n",
       "         59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,\n",
       "         71.,  72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  81.,  82.,\n",
       "         83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
       "         95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105., 106.,\n",
       "        107., 108., 109., 110., 111., 112., 113.,   1.,   1.,   1.,   1.,   1.,\n",
       "          1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_nsp(torch.arange(0, 114).to(device), 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7440, -1.6517, -0.7186, -1.8897],\n",
       "         [-0.3794,  2.4063,  1.8389, -1.0588],\n",
       "         [-0.1331,  0.6404, -1.2773,  0.9912],\n",
       "         [-1.3810,  0.1835, -0.8249, -0.3183],\n",
       "         [-1.0342, -1.0985, -0.7125,  0.9913],\n",
       "         [ 0.4465,  0.3917,  0.6880,  0.5020],\n",
       "         [-1.2138,  0.6493,  1.3062,  1.5970],\n",
       "         [ 1.0245,  0.1289, -0.0637, -0.8690],\n",
       "         [-0.8740, -1.3837, -0.8737, -0.9552],\n",
       "         [-0.4895,  0.0757,  0.1145,  0.5715],\n",
       "         [ 0.7882,  0.0627,  0.4653, -1.2508],\n",
       "         [ 1.9917, -0.0231, -2.0122,  0.8020],\n",
       "         [-0.0132, -0.5354, -0.7686,  0.4104],\n",
       "         [ 0.2389,  0.1792,  2.2009,  0.2688],\n",
       "         [-0.7481, -0.9645, -0.1956,  0.4764],\n",
       "         [-0.3492, -0.8736,  2.0249,  1.1491],\n",
       "         [ 2.1027,  1.3551, -0.9135,  0.4117],\n",
       "         [-0.8189,  0.8682,  0.4944, -1.3722],\n",
       "         [ 0.6548, -2.3145, -2.9876,  1.1823],\n",
       "         [ 0.0463, -0.7179, -0.6611, -1.3129],\n",
       "         [-1.7585, -0.3310,  0.7215, -0.0833],\n",
       "         [ 1.0794, -1.6389,  1.5151,  0.0128],\n",
       "         [-0.2683,  0.3174,  1.0601,  1.0186],\n",
       "         [-0.2844,  0.8315, -1.5733, -1.6588],\n",
       "         [ 1.4878,  1.9793,  0.4267,  0.7464],\n",
       "         [ 1.2237,  0.6909, -0.5510,  0.7337],\n",
       "         [ 1.5316,  1.1422,  0.5075, -1.5351],\n",
       "         [-0.6388, -1.5655,  0.8195, -0.5381],\n",
       "         [ 1.3039,  0.0903,  0.9046,  1.4698],\n",
       "         [-0.6369,  0.7613,  0.9598,  0.9663],\n",
       "         [-0.5658, -0.4284,  0.2026, -0.2811],\n",
       "         [-0.2679, -0.9458, -1.1602,  1.1026],\n",
       "         [ 0.8038,  0.8210, -1.1130, -0.5251],\n",
       "         [ 0.0649,  0.8386, -1.1803, -0.1543],\n",
       "         [ 0.8416, -0.0277,  1.0917, -0.3469],\n",
       "         [-1.8367, -0.3706, -0.2721,  1.1322],\n",
       "         [ 0.8504, -0.5308,  0.2855,  0.0916],\n",
       "         [-1.3726, -0.9668, -0.3496, -0.2746],\n",
       "         [ 0.1912, -0.5686,  0.6889, -0.1181],\n",
       "         [-0.1053,  0.5398, -0.5484,  0.9631],\n",
       "         [ 0.1138, -0.2057,  0.1557,  0.0149],\n",
       "         [-0.3921, -0.7277,  0.4302,  0.8178],\n",
       "         [-0.5493,  0.0504, -0.8962, -2.0697],\n",
       "         [-0.6989,  0.9725, -0.0595,  0.0478],\n",
       "         [ 1.8395,  0.5128, -0.9655,  1.0283],\n",
       "         [-0.2358, -0.4028, -0.3239,  0.8556],\n",
       "         [ 0.3417,  0.3383, -0.5984,  2.3710],\n",
       "         [-0.3129,  1.2122, -0.4422,  1.6947],\n",
       "         [-1.4879, -1.6926,  1.2808, -0.0285],\n",
       "         [ 1.9987,  0.0244, -0.1981, -0.5158],\n",
       "         [-1.3200, -0.7648, -0.0440, -0.2642],\n",
       "         [-0.8520, -0.5847, -0.6567, -0.1387],\n",
       "         [ 0.9612,  1.9674,  0.4608,  0.5755],\n",
       "         [-2.4761, -1.0149,  1.2485, -0.6730],\n",
       "         [ 0.3430,  0.0706, -0.6004,  0.0750],\n",
       "         [ 0.4249,  0.6971,  0.2127,  0.7547],\n",
       "         [ 0.1563,  0.9323,  0.2134, -0.5982],\n",
       "         [-1.7102,  0.2107,  0.8072, -0.0185],\n",
       "         [-0.3202, -0.4278, -0.4856,  0.5347],\n",
       "         [ 1.0477, -0.2385,  0.4964, -0.5019],\n",
       "         [-1.0253, -0.4250, -0.1969, -0.1599],\n",
       "         [-0.5979, -0.1953, -0.2416, -1.8308],\n",
       "         [-0.0917,  0.0787,  1.0718, -1.0168],\n",
       "         [-0.6835, -1.2649,  0.8125, -0.8829],\n",
       "         [ 0.3884, -0.1744,  0.3492, -0.9011],\n",
       "         [-0.4285,  1.2642, -0.0733, -1.0197],\n",
       "         [-1.1810, -1.1524, -0.9543,  1.9737],\n",
       "         [ 0.9701, -0.3841, -0.6921,  1.8440],\n",
       "         [-0.3090, -0.2671, -0.3517,  0.5767],\n",
       "         [ 0.1160,  0.3544,  0.5913,  0.5501],\n",
       "         [-1.4612,  0.2493, -0.6482,  1.5323],\n",
       "         [ 0.4313, -0.6008,  0.0210, -1.2686],\n",
       "         [ 0.1522,  0.9103, -0.9465, -0.2050],\n",
       "         [-0.7552, -0.8118,  0.3340,  1.2032],\n",
       "         [ 0.7645, -0.1511, -0.0308, -0.0939],\n",
       "         [-0.7222, -0.2249, -1.9998,  0.5566],\n",
       "         [ 0.3845,  0.0854, -1.4070, -0.9316],\n",
       "         [-0.4213,  0.5434, -0.2418, -0.1696],\n",
       "         [-0.1945, -0.9708, -1.0398, -1.7028],\n",
       "         [ 0.4049,  0.5367,  0.1577,  0.1427],\n",
       "         [ 1.6959,  0.5092, -1.0695, -1.7375],\n",
       "         [-0.5055, -0.9215,  0.2984,  0.0346],\n",
       "         [ 0.1064,  1.2855, -0.4480,  0.3063],\n",
       "         [ 0.0971,  3.6590, -0.7103,  0.7535],\n",
       "         [-0.7148,  0.4569, -0.6525,  0.4146],\n",
       "         [-0.2714, -1.0520, -0.1011, -0.2249],\n",
       "         [ 1.6617,  1.1265,  1.2218,  0.1505],\n",
       "         [-0.3633, -0.7148, -0.6480, -1.5131],\n",
       "         [-1.5368, -0.1540,  0.2021,  0.9852],\n",
       "         [-0.7825,  0.0552,  2.7128, -0.3447],\n",
       "         [-3.0141,  1.3546,  0.5493, -0.9367],\n",
       "         [ 0.8007,  1.0535, -0.1103,  0.6724],\n",
       "         [ 1.0327,  0.3261,  0.5704, -0.9533],\n",
       "         [ 0.1550, -1.7206, -1.6435, -0.1964],\n",
       "         [-0.1569, -0.8494, -1.7220, -0.5652],\n",
       "         [ 1.3453,  1.2491, -1.1477, -0.7850],\n",
       "         [ 1.4732,  0.4898, -1.9885, -0.1841],\n",
       "         [ 1.1093, -0.2734, -0.9531, -1.5487],\n",
       "         [ 1.7800,  0.5875,  0.9175,  0.4932],\n",
       "         [ 1.5150, -0.1251,  0.3564, -0.1199],\n",
       "         [-0.0630,  0.0938, -1.2190,  0.6721],\n",
       "         [ 2.3284, -0.3886,  0.8076, -1.5914],\n",
       "         [ 0.4131,  1.3080,  0.9188, -0.2454],\n",
       "         [-0.0861, -0.3989,  0.5285,  1.1286],\n",
       "         [-2.4925,  0.5105,  0.8560, -0.7989],\n",
       "         [-1.0752,  1.0031,  0.4616,  0.4932],\n",
       "         [ 0.4691,  0.8094, -1.0023, -0.9651],\n",
       "         [-0.2226, -0.5214, -0.7572, -0.2794],\n",
       "         [-1.7807, -1.6119,  0.8159,  1.0453],\n",
       "         [ 0.4397, -1.9191, -1.3776, -1.6539],\n",
       "         [-0.2904, -0.3877, -0.3756, -0.3064],\n",
       "         [-1.4086, -0.6209, -1.1646, -0.4305],\n",
       "         [-0.4251, -1.3741, -0.8126,  2.4617],\n",
       "         [-0.6768, -1.1127, -0.4906,  1.4726],\n",
       "         [ 0.8035, -1.4217, -1.5650, -0.0798],\n",
       "         [-1.2676,  0.9238,  0.1502, -0.0864],\n",
       "         [ 0.2618,  0.4635, -1.4002, -0.0662],\n",
       "         [ 1.0627, -0.3469,  0.5547,  1.1986],\n",
       "         [ 1.0825,  0.7287,  1.6721, -1.6461],\n",
       "         [ 0.1442, -0.8505, -1.2859,  1.0590],\n",
       "         [-0.6135, -0.0157, -1.0110,  1.1794],\n",
       "         [-0.9386,  0.1894,  0.9770, -0.1116],\n",
       "         [-1.1431, -0.8466, -0.5802, -0.1593],\n",
       "         [ 1.7154, -0.1735,  0.2338,  0.5573],\n",
       "         [ 0.4969,  0.3008,  1.6822,  0.7522],\n",
       "         [-1.2092, -0.6613,  0.2677, -0.0914],\n",
       "         [-0.1375,  0.0315,  1.8467,  1.5148],\n",
       "         [ 0.8034, -0.2326,  0.8317, -0.9711]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(0, 128).unsqueeze(0)\n",
    "torch.manual_seed(114514)\n",
    "Bert = AbsolutePositionEmbedding(2, 128)\n",
    "b = Bert(a)\n",
    "Bert = BertEmbeddings(400, 3)\n",
    "c = Bert(a)\n",
    "c\n",
    "Bert = DEBertAEmbeddings(400, 128)\n",
    "c = Bert(a)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6500e-01, -4.2874e-01],\n",
       "         [-2.4267e-02,  1.4942e-01],\n",
       "         [-7.0685e-02,  2.1661e-01],\n",
       "         [-1.0899e+00,  8.9571e-01],\n",
       "         [-3.2904e-01,  1.1737e+00],\n",
       "         [ 4.5906e-01, -2.0881e+00],\n",
       "         [-6.8765e-01, -3.2952e-01],\n",
       "         [-1.7524e-01,  8.3306e-01],\n",
       "         [-1.6071e+00, -4.6016e-01],\n",
       "         [ 1.3175e-01,  1.3799e+00],\n",
       "         [ 1.0430e+00,  1.2995e+00],\n",
       "         [ 1.9181e+00,  4.7578e-01],\n",
       "         [ 8.9001e-01, -1.1867e+00],\n",
       "         [-3.6335e-01, -1.0446e+00],\n",
       "         [ 8.6211e-01, -5.1553e-01],\n",
       "         [-7.7988e-01, -1.8068e+00],\n",
       "         [ 1.4480e+00,  1.1284e+00],\n",
       "         [-5.3514e-01,  1.3089e+00],\n",
       "         [ 5.4544e-01,  1.4012e+00],\n",
       "         [ 1.4425e+00,  8.3782e-01],\n",
       "         [-1.1620e-01,  9.1472e-01],\n",
       "         [-2.4817e-01, -5.4873e-01],\n",
       "         [ 9.2141e-01, -1.0283e+00],\n",
       "         [-9.4015e-01, -6.1165e-01],\n",
       "         [ 5.2813e-01,  7.4984e-01],\n",
       "         [ 2.1591e-01,  8.1243e-01],\n",
       "         [-1.5864e+00,  9.2640e-01],\n",
       "         [ 2.7632e+00,  1.2123e-01],\n",
       "         [-2.1887e+00, -8.7876e-02],\n",
       "         [-5.9036e-01, -1.2117e-02],\n",
       "         [ 1.4901e+00,  5.2172e-01],\n",
       "         [ 5.8583e-01, -8.3429e-01],\n",
       "         [-4.4251e-01, -6.1761e-01],\n",
       "         [ 1.4393e-01,  1.9546e+00],\n",
       "         [-5.2858e-01, -7.6557e-01],\n",
       "         [-1.0567e+00, -1.7481e+00],\n",
       "         [-1.4321e+00,  8.5906e-01],\n",
       "         [-8.3841e-01,  1.9371e-01],\n",
       "         [-2.7261e-01,  5.5255e-01],\n",
       "         [-6.4872e-01, -1.9827e+00],\n",
       "         [-7.2577e-02,  2.2977e+00],\n",
       "         [-8.3903e-01, -4.0360e-01],\n",
       "         [-5.7070e-01,  1.8608e-01],\n",
       "         [ 1.5669e+00,  6.0858e-01],\n",
       "         [ 1.2237e+00, -8.2568e-01],\n",
       "         [ 3.4360e-01, -1.1682e+00],\n",
       "         [ 6.2694e-01, -6.7698e-02],\n",
       "         [ 3.6742e-01, -5.8875e-01],\n",
       "         [ 6.1122e-01,  7.9999e-03],\n",
       "         [ 9.2962e-01, -9.6138e-02],\n",
       "         [ 3.2429e-01, -9.7907e-01],\n",
       "         [-1.1057e+00,  1.6828e-03],\n",
       "         [ 1.4310e+00, -3.5994e-01],\n",
       "         [ 9.1209e-01, -1.9747e+00],\n",
       "         [-1.3163e+00,  1.1743e-01],\n",
       "         [-5.4523e-01, -1.7878e+00],\n",
       "         [-3.3735e-01, -5.5826e-01],\n",
       "         [ 2.1600e+00,  5.8108e-01],\n",
       "         [ 4.7638e-01, -2.8861e-01],\n",
       "         [-1.7191e+00, -1.6082e-01],\n",
       "         [-6.9147e-01,  3.4914e-01],\n",
       "         [ 2.5092e-01,  4.8028e-01],\n",
       "         [-2.5320e+00, -9.0174e-01],\n",
       "         [ 8.6250e-01,  4.6539e-01],\n",
       "         [-1.1505e+00,  2.3824e-01],\n",
       "         [ 4.6087e-01, -1.5439e+00],\n",
       "         [ 5.1738e-01, -5.1964e-01],\n",
       "         [ 8.8978e-01, -4.8855e-01],\n",
       "         [-6.1656e-01, -2.7760e-01],\n",
       "         [ 2.9811e-01,  1.0800e+00],\n",
       "         [ 1.1979e+00, -2.8736e-01],\n",
       "         [ 6.5282e-01,  1.1187e-01],\n",
       "         [-1.1386e+00,  1.4632e-01],\n",
       "         [ 4.4802e-02, -9.4296e-01],\n",
       "         [ 1.1630e+00,  3.9781e-01],\n",
       "         [ 7.8356e-01,  2.4599e-01],\n",
       "         [-2.3973e-01, -1.4374e+00],\n",
       "         [-2.4402e+00,  3.1478e+00],\n",
       "         [-7.4100e-01, -5.0482e-01],\n",
       "         [ 1.5380e-01, -1.0995e+00],\n",
       "         [-1.7457e-01,  1.2145e+00],\n",
       "         [ 3.5991e-01, -1.5879e-01],\n",
       "         [-1.9075e-01, -8.6816e-01],\n",
       "         [-5.0350e-01, -3.3051e+00],\n",
       "         [ 1.1159e+00,  2.5610e+00],\n",
       "         [ 4.0922e-01,  1.9438e-01],\n",
       "         [-9.8604e-01,  6.8313e-01],\n",
       "         [ 3.3785e-01, -1.9596e+00],\n",
       "         [ 1.0680e+00, -7.0142e-01],\n",
       "         [ 2.7690e-01,  7.5430e-01],\n",
       "         [ 6.4625e-01,  1.5496e-02],\n",
       "         [ 1.0293e+00, -2.0321e+00],\n",
       "         [ 7.4066e-01, -1.2242e+00],\n",
       "         [-1.9006e+00, -4.1521e-01],\n",
       "         [ 2.6778e-01, -1.2414e-01],\n",
       "         [-1.6223e-01, -1.1844e+00],\n",
       "         [ 4.0940e-01, -7.2866e-01],\n",
       "         [-5.1456e-02,  9.4459e-01],\n",
       "         [-1.6063e-01,  2.7909e-01],\n",
       "         [ 8.2729e-01, -5.6155e-01],\n",
       "         [-7.7841e-02, -3.9836e-01],\n",
       "         [ 3.7123e-01, -1.6140e+00],\n",
       "         [ 1.1622e-01, -1.3453e+00],\n",
       "         [-4.2050e-01, -7.2549e-01],\n",
       "         [-1.0654e+00, -1.0480e+00],\n",
       "         [ 1.2744e-01, -8.4875e-01],\n",
       "         [-1.0321e+00,  5.3634e-01],\n",
       "         [ 3.6927e-01,  1.0726e+00],\n",
       "         [-6.3547e-01,  1.5019e+00],\n",
       "         [-1.8970e+00,  1.4158e+00],\n",
       "         [ 1.0953e+00, -1.6201e+00],\n",
       "         [-1.0537e+00,  3.2987e-02],\n",
       "         [-1.8878e-01, -9.9465e-02],\n",
       "         [-5.2893e-01,  5.3157e-01],\n",
       "         [-4.2965e-02,  2.5514e-01],\n",
       "         [ 1.3838e+00, -1.1227e+00],\n",
       "         [ 1.0380e+00,  3.5282e-02],\n",
       "         [ 9.1263e-01, -7.9211e-01],\n",
       "         [-3.1269e+00,  5.0995e-01],\n",
       "         [-1.2997e+00,  8.4554e-01],\n",
       "         [-8.3665e-01,  2.8409e-01],\n",
       "         [ 5.5257e-02, -5.1005e-01],\n",
       "         [-1.5324e-01,  2.5530e+00],\n",
       "         [ 2.1643e-01, -1.9828e-01],\n",
       "         [ 2.1110e+00, -1.3150e+00],\n",
       "         [-2.3707e-01,  1.6417e+00],\n",
       "         [-1.2432e+00,  8.1703e-01],\n",
       "         [-7.8730e-01, -1.9950e-01]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6500e-01, -4.2874e-01,  8.4829e-01,  1.0958e+00,  3.9295e-01],\n",
       "         [-2.4267e-02,  1.4942e-01,  1.6070e+00, -1.4871e+00,  4.5711e-01],\n",
       "         [-7.0685e-02,  2.1661e-01,  4.2503e-01,  7.3498e-01, -1.0292e+00],\n",
       "         [-1.0899e+00,  8.9571e-01,  7.1001e-01,  7.9472e-01, -8.1961e-01],\n",
       "         [-3.2904e-01,  1.1737e+00, -5.3887e-01,  1.3320e+00, -1.0281e+00],\n",
       "         [ 4.5906e-01, -2.0881e+00, -1.4809e+00, -4.3133e-01,  3.5989e-01],\n",
       "         [-6.8765e-01, -3.2952e-01, -1.0596e-01,  1.3604e+00,  1.8881e-01],\n",
       "         [-1.7524e-01,  8.3306e-01,  1.0397e+00, -6.9338e-01, -1.2310e+00],\n",
       "         [-1.6071e+00, -4.6016e-01,  6.5157e-01, -2.9964e-01,  8.4878e-02],\n",
       "         [ 1.3175e-01,  1.3799e+00, -2.0456e+00,  4.4319e-01,  1.1152e+00],\n",
       "         [ 1.0430e+00,  1.2995e+00,  1.6779e+00, -4.2784e-01,  1.7992e+00],\n",
       "         [ 1.9181e+00,  4.7578e-01, -2.6459e-01,  4.6090e-01,  2.3222e+00],\n",
       "         [ 8.9001e-01, -1.1867e+00,  3.4631e-01, -6.7855e-01,  3.3243e-01],\n",
       "         [-3.6335e-01, -1.0446e+00, -8.2044e-01,  4.4227e-02, -5.2708e-01],\n",
       "         [ 8.6211e-01, -5.1553e-01,  2.9942e-01,  7.9667e-01, -2.1523e-01],\n",
       "         [-7.7988e-01, -1.8068e+00,  5.3477e-01,  9.7487e-01,  3.8068e-01],\n",
       "         [ 1.4480e+00,  1.1284e+00, -3.4008e-01,  2.4893e-01, -2.0409e-01],\n",
       "         [-5.3514e-01,  1.3089e+00, -1.5442e+00, -2.2405e-01, -4.5966e-01],\n",
       "         [ 5.4544e-01,  1.4012e+00, -7.6353e-01,  3.0748e-01, -1.4142e-01],\n",
       "         [ 1.4425e+00,  8.3782e-01,  2.1949e-02, -1.1896e+00,  5.9344e-01],\n",
       "         [-1.1620e-01,  9.1472e-01, -7.9771e-01, -2.4879e-01, -4.3801e-01],\n",
       "         [-2.4817e-01, -5.4873e-01, -1.2929e+00,  2.6681e-01,  3.2120e-01],\n",
       "         [ 9.2141e-01, -1.0283e+00,  8.7826e-01,  4.8021e-01, -7.2634e-01],\n",
       "         [-9.4015e-01, -6.1165e-01,  1.4519e+00,  4.0496e-02,  7.6393e-01],\n",
       "         [ 5.2813e-01,  7.4984e-01, -1.3286e+00,  6.8702e-01, -2.9413e+00],\n",
       "         [ 2.1591e-01,  8.1243e-01, -2.2972e-01, -1.8047e+00, -1.1126e+00],\n",
       "         [-1.5864e+00,  9.2640e-01,  6.6195e-01,  4.7579e-01, -1.2776e+00],\n",
       "         [ 2.7632e+00,  1.2123e-01, -9.5192e-01, -9.5377e-01,  4.2142e-01],\n",
       "         [-2.1887e+00, -8.7876e-02, -1.7425e+00,  2.4535e-01, -1.7683e-01],\n",
       "         [-5.9036e-01, -1.2117e-02, -6.8801e-01, -4.2211e-01,  1.1645e+00],\n",
       "         [ 1.4901e+00,  5.2172e-01,  1.2557e+00,  1.8282e-01, -2.8345e-02],\n",
       "         [ 5.8583e-01, -8.3429e-01,  4.9606e-01,  1.9453e+00, -1.2597e+00],\n",
       "         [-4.4251e-01, -6.1761e-01, -1.5363e+00, -1.1237e+00, -1.9495e+00],\n",
       "         [ 1.4393e-01,  1.9546e+00, -7.3209e-01, -4.1308e-01,  2.5452e-01],\n",
       "         [-5.2858e-01, -7.6557e-01,  2.1809e-01,  2.0926e+00,  1.7207e-01],\n",
       "         [-1.0567e+00, -1.7481e+00, -1.7491e+00,  9.4107e-01, -6.4034e-01],\n",
       "         [-1.4321e+00,  8.5906e-01,  1.1052e+00, -2.1622e-01,  2.5666e-01],\n",
       "         [-8.3841e-01,  1.9371e-01,  4.8698e-01,  3.2540e-01,  8.0634e-01],\n",
       "         [-2.7261e-01,  5.5255e-01,  2.1936e+00,  1.1552e+00,  6.7751e-01],\n",
       "         [-6.4872e-01, -1.9827e+00,  3.3769e-01, -9.9968e-01,  2.8126e-02],\n",
       "         [-7.2577e-02,  2.2977e+00, -1.0359e+00, -4.8490e-01,  7.0190e-01],\n",
       "         [-8.3903e-01, -4.0360e-01, -4.3230e-02, -1.7440e+00, -6.8420e-01],\n",
       "         [-5.7070e-01,  1.8608e-01,  5.0548e-01,  1.5764e+00, -4.0368e-01],\n",
       "         [ 1.5669e+00,  6.0858e-01,  1.5940e+00, -4.0126e-01,  7.2041e-01],\n",
       "         [ 1.2237e+00, -8.2568e-01,  1.4971e-01, -6.1116e-01,  4.4825e-01],\n",
       "         [ 3.4360e-01, -1.1682e+00,  1.3506e+00, -9.9338e-01,  1.0874e+00],\n",
       "         [ 6.2694e-01, -6.7698e-02, -2.3352e-01,  5.1481e-01,  4.5086e-01],\n",
       "         [ 3.6742e-01, -5.8875e-01,  7.8657e-01,  2.6647e-01, -1.7722e+00],\n",
       "         [ 6.1122e-01,  7.9999e-03, -4.8505e-01,  3.4661e-01,  3.6385e-01],\n",
       "         [ 9.2962e-01, -9.6138e-02, -9.0264e-01,  4.4038e-01,  1.4197e-01],\n",
       "         [ 3.2429e-01, -9.7907e-01, -3.9881e-01, -7.8990e-01,  3.5834e-01],\n",
       "         [-1.1057e+00,  1.6828e-03,  8.0450e-01, -1.2395e+00,  9.5638e-02],\n",
       "         [ 1.4310e+00, -3.5994e-01, -1.2705e+00, -4.0898e-01, -2.3053e-01],\n",
       "         [ 9.1209e-01, -1.9747e+00, -8.0195e-01,  2.0089e+00,  7.3967e-01],\n",
       "         [-1.3163e+00,  1.1743e-01,  9.3800e-01,  2.9898e-01, -4.0500e-02],\n",
       "         [-5.4523e-01, -1.7878e+00,  1.6679e+00, -1.9955e+00, -5.1971e-01],\n",
       "         [-3.3735e-01, -5.5826e-01, -2.9756e-01,  5.9493e-01, -1.1656e+00],\n",
       "         [ 2.1600e+00,  5.8108e-01,  1.9314e+00, -6.1777e-01, -3.5175e-01],\n",
       "         [ 4.7638e-01, -2.8861e-01, -9.1361e-01, -4.4669e-01, -1.1140e+00],\n",
       "         [-1.7191e+00, -1.6082e-01, -1.7005e+00,  1.4938e+00, -8.9947e-01],\n",
       "         [-6.9147e-01,  3.4914e-01, -5.9028e-01,  4.9268e-01, -8.3711e-01],\n",
       "         [ 2.5092e-01,  4.8028e-01, -2.5066e+00,  1.7993e+00, -1.1446e-01],\n",
       "         [-2.5320e+00, -9.0174e-01, -1.3996e-01, -4.8525e-01,  2.4209e-01],\n",
       "         [ 8.6250e-01,  4.6539e-01,  1.3771e+00,  9.2378e-01,  4.6881e-01],\n",
       "         [-1.1505e+00,  2.3824e-01,  3.2747e-01, -1.4410e+00, -1.0563e+00],\n",
       "         [ 4.6087e-01, -1.5439e+00,  1.4202e+00,  3.4031e-01, -3.2383e-01],\n",
       "         [ 5.1738e-01, -5.1964e-01,  9.3929e-01, -9.4071e-01, -3.1758e-02],\n",
       "         [ 8.8978e-01, -4.8855e-01,  9.8521e-01, -5.2637e-02,  4.2542e-01],\n",
       "         [-6.1656e-01, -2.7760e-01,  5.8880e-01,  5.5640e-01, -8.7110e-01],\n",
       "         [ 2.9811e-01,  1.0800e+00, -4.2725e-01,  1.1522e-01, -8.6553e-01],\n",
       "         [ 1.1979e+00, -2.8736e-01,  1.1244e+00, -2.9572e-01, -1.5072e-01],\n",
       "         [ 6.5282e-01,  1.1187e-01,  2.3260e-01, -5.9656e-01,  7.4874e-01],\n",
       "         [-1.1386e+00,  1.4632e-01,  1.5951e-01, -1.6914e+00, -1.2096e+00],\n",
       "         [ 4.4802e-02, -9.4296e-01, -1.3662e+00,  4.5627e-01, -6.3936e-01],\n",
       "         [ 1.1630e+00,  3.9781e-01, -9.8970e-01, -8.3909e-01,  1.5737e+00],\n",
       "         [ 7.8356e-01,  2.4599e-01, -1.0619e+00,  1.8699e+00, -2.4856e-02],\n",
       "         [-2.3973e-01, -1.4374e+00, -7.1118e-01, -2.2604e+00, -1.7880e+00],\n",
       "         [-2.4402e+00,  3.1478e+00, -8.5324e-01,  1.0847e+00,  4.2478e-01],\n",
       "         [-7.4100e-01, -5.0482e-01, -1.2504e+00,  6.3539e-01, -5.7859e-02],\n",
       "         [ 1.5380e-01, -1.0995e+00, -3.3793e-01, -9.1262e-01,  1.2745e+00],\n",
       "         [-1.7457e-01,  1.2145e+00,  3.0295e-01,  1.0721e+00, -1.8473e+00],\n",
       "         [ 3.5991e-01, -1.5879e-01, -6.8618e-01, -4.1332e-01, -2.5271e-01],\n",
       "         [-1.9075e-01, -8.6816e-01, -1.6770e-01,  2.6874e-01, -1.2931e+00],\n",
       "         [-5.0350e-01, -3.3051e+00, -4.7286e-01,  3.1480e+00, -1.1692e+00],\n",
       "         [ 1.1159e+00,  2.5610e+00,  2.0747e+00, -8.3627e-01, -1.6613e+00],\n",
       "         [ 4.0922e-01,  1.9438e-01,  6.8466e-01, -1.5305e+00, -1.5641e-01],\n",
       "         [-9.8604e-01,  6.8313e-01,  1.2478e-01, -2.1567e-01, -1.1074e+00],\n",
       "         [ 3.3785e-01, -1.9596e+00, -8.4803e-01, -1.1881e-01, -1.9116e-01],\n",
       "         [ 1.0680e+00, -7.0142e-01, -1.5496e+00, -1.2879e+00, -1.1316e+00],\n",
       "         [ 2.7690e-01,  7.5430e-01,  3.8835e-01,  4.8955e-01, -4.9799e-01],\n",
       "         [ 6.4625e-01,  1.5496e-02, -1.5105e+00,  7.7203e-01,  3.2166e-01],\n",
       "         [ 1.0293e+00, -2.0321e+00, -3.0394e+00, -2.2576e-01,  1.7138e-01],\n",
       "         [ 7.4066e-01, -1.2242e+00, -2.4031e-01, -8.7959e-01, -1.0957e+00],\n",
       "         [-1.9006e+00, -4.1521e-01, -1.1039e-01,  8.4901e-01,  2.3479e-01],\n",
       "         [ 2.6778e-01, -1.2414e-01,  8.4522e-01, -5.4291e-01,  1.9250e+00],\n",
       "         [-1.6223e-01, -1.1844e+00, -5.4058e-01, -1.4422e+00, -2.4460e-01],\n",
       "         [ 4.0940e-01, -7.2866e-01,  1.1017e+00,  1.4067e+00,  1.1091e+00],\n",
       "         [-5.1456e-02,  9.4459e-01, -2.7048e+00,  7.8363e-01,  1.1550e+00],\n",
       "         [-1.6063e-01,  2.7909e-01, -1.7225e+00,  3.6511e-01,  6.3462e-01],\n",
       "         [ 8.2729e-01, -5.6155e-01, -3.2242e-01,  1.3135e+00,  1.5892e+00],\n",
       "         [-7.7841e-02, -3.9836e-01, -1.2072e+00,  1.4652e+00,  7.3390e-01],\n",
       "         [ 3.7123e-01, -1.6140e+00, -8.3838e-01, -8.7402e-01, -5.1888e-01],\n",
       "         [ 1.1622e-01, -1.3453e+00, -4.3796e-01, -2.4331e-01, -3.2655e-01],\n",
       "         [-4.2050e-01, -7.2549e-01,  1.1461e+00,  1.7544e-01, -3.8728e-01],\n",
       "         [-1.0654e+00, -1.0480e+00, -2.0513e-01, -1.5580e-01,  3.3422e-01],\n",
       "         [ 1.2744e-01, -8.4875e-01,  6.2200e-01, -3.7916e-01, -7.2784e-01],\n",
       "         [-1.0321e+00,  5.3634e-01, -7.4076e-01, -1.7498e-01, -4.8859e-01],\n",
       "         [ 3.6927e-01,  1.0726e+00, -1.6746e+00, -1.2660e+00, -1.8826e+00],\n",
       "         [-6.3547e-01,  1.5019e+00,  1.7683e-01, -5.0804e-01, -1.6525e+00],\n",
       "         [-1.8970e+00,  1.4158e+00,  8.5006e-02,  1.1404e+00, -5.5152e-02],\n",
       "         [ 1.0953e+00, -1.6201e+00, -3.5353e-01, -8.4252e-01, -3.1325e-01],\n",
       "         [-1.0537e+00,  3.2987e-02, -6.5045e-01, -2.6878e+00, -1.0200e+00],\n",
       "         [-1.8878e-01, -9.9465e-02, -8.8928e-01, -2.5670e-01,  1.6584e-01],\n",
       "         [-5.2893e-01,  5.3157e-01,  1.6891e+00, -6.6453e-01,  1.2242e-01],\n",
       "         [-4.2965e-02,  2.5514e-01,  7.9382e-01,  3.3991e-01,  9.3752e-01],\n",
       "         [ 1.3838e+00, -1.1227e+00, -1.1491e+00,  2.8763e-01,  3.3943e-01],\n",
       "         [ 1.0380e+00,  3.5282e-02, -2.1948e+00, -8.8944e-01,  4.1347e-01],\n",
       "         [ 9.1263e-01, -7.9211e-01, -6.5018e-01, -4.3482e-01, -5.1677e-01],\n",
       "         [-3.1269e+00,  5.0995e-01,  3.2912e-01,  4.8154e-02,  2.1955e+00],\n",
       "         [-1.2997e+00,  8.4554e-01,  1.3025e+00,  1.1612e+00,  1.7872e+00],\n",
       "         [-8.3665e-01,  2.8409e-01, -4.7259e-01,  6.2412e-01,  2.0594e+00],\n",
       "         [ 5.5257e-02, -5.1005e-01,  1.2938e+00,  1.4281e+00, -2.0422e-01],\n",
       "         [-1.5324e-01,  2.5530e+00,  9.6420e-01, -6.2666e-01,  2.4684e-01],\n",
       "         [ 2.1643e-01, -1.9828e-01,  6.8217e-01, -6.6648e-01, -2.0848e+00],\n",
       "         [ 2.1110e+00, -1.3150e+00,  1.6303e-01,  9.0841e-01, -1.0131e+00],\n",
       "         [-2.3707e-01,  1.6417e+00,  3.8592e-01, -1.1933e+00,  1.6796e-01],\n",
       "         [-1.2432e+00,  8.1703e-01, -4.3814e-01,  1.6185e-01, -1.2413e+00],\n",
       "         [-7.8730e-01, -1.9950e-01,  8.1262e-02,  3.5207e-01, -1.9040e+00]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.cat((b, c), dim = 2)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module): # for BERT and ALBERT\n",
    "    def __init__(self, n_embed, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create a matrix of shape (max_len, d_model) with positional encodings\n",
    "        pe = torch.zeros(max_seq_len, n_embed)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Div term represents the frequency of the sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, n_embed, 2).float() * (-torch.log(torch.tensor(10000.0)) / n_embed))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add a batch dimension for broadcasting\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        # Register pe as a buffer, which means it's not a parameter but should be part of the state\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(n_embed, eps=1e-12, elementwise_affine=True) # eps: added as sqrt(var + eps) to prevent zero denominator\n",
    "        self.dropout = torch.nn.Dropout(p=0.1, inplace=False) # inplace=False: do not replace the input by dropouted input\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input embeddings\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        embeddings = self.layer_norm(x)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class AbsolutePositionEmbedding(torch.nn.Module): # for DEBERTA\n",
    "    def __init__(self, n_embed, max_seq_len):\n",
    "        super(AbsolutePositionEmbedding, self).__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, n_embed)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # Get the position IDs from the input IDs\n",
    "        position_ids = torch.arange(0, self.max_seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        # Get the position embeddings\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        return position_embeddings\n",
    "    \n",
    "class BertEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed): # n_embed = 3, max_seq_len = 16\n",
    "        super().__init__()\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, n_embed) # number of words is length of text, each words has length n_embed\n",
    "\n",
    "    def forward(self, x):\n",
    "        words_embeddings = self.word_embeddings(x)\n",
    "\n",
    "        return words_embeddings\n",
    "    \n",
    "class ALBertEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed, n_hid = 3): # n_embed = 3\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_embeddings = torch.nn.Embedding(vocab_size, n_hid) # number of words is length of text, each words has length n_embed\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(n_hid, n_embed) # number of words is length of text, each words has length n_embed\n",
    "    def forward(self, x):\n",
    "        hid_embeddings = self.hid_embeddings(x)\n",
    "        \n",
    "        words_embeddings = self.word_embeddings(hid_embeddings)\n",
    "\n",
    "        return words_embeddings\n",
    "    \n",
    "class DEBertAEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, n_embed_word = 3, n_embed_p = 1): \n",
    "        super().__init__()\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, n_embed_word) # number of words is length of text, each words has length n_embed\n",
    "\n",
    "        self.abposit_embeddings = AbsolutePositionEmbedding(n_embed_p, max_seq_len)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        words_embeddings = self.word_embeddings(x)\n",
    "        \n",
    "        abposits_embeddings = self.abposit_embeddings(x)\n",
    "\n",
    "        return torch.cat((words_embeddings, abposits_embeddings), dim = 2)\n",
    "\n",
    "\n",
    "class BertAttentionHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head in MultiHeaded Self Attention layer.\n",
    "    The idea is identical to the original paper (\"Attention is all you need\"),\n",
    "    however instead of implementing multiple heads to be evaluated in parallel we matrix multiplication,\n",
    "    separated in a distinct class for easier and clearer interpretability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, dropout, n_embed): # dropout = 0.1, n_embed = 3\n",
    "        super().__init__()\n",
    "\n",
    "        self.query = torch.nn.Linear(in_features=n_embed, out_features=head_size)\n",
    "        self.key = torch.nn.Linear(in_features=n_embed, out_features=head_size)\n",
    "        self.values = torch.nn.Linear(in_features=n_embed, out_features=head_size)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # B, Seq_len, N_embed\n",
    "        B, seq_len, n_embed = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.values(x)\n",
    "\n",
    "        weights = (q @ k.transpose(-2, -1)) / math.sqrt(n_embed)  # (B, Seq_len, Seq_len)\n",
    "        weights = weights.masked_fill(mask == 0, -1e9)  # mask out not attended tokens\n",
    "\n",
    "        scores = F.softmax(weights, dim=-1)\n",
    "        scores = self.dropout(scores)\n",
    "\n",
    "        context = scores @ v\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class BertSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    MultiHeaded Self-Attention mechanism as described in \"Attention is all you need\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, dropout, n_embed): # , n_heads = 1, dropout = 0.1, n_embed = 3\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        n_heads = n_heads\n",
    "\n",
    "        self.heads = torch.nn.ModuleList([BertAttentionHead(head_size, dropout, n_embed) for _ in range(n_heads)])\n",
    "\n",
    "        self.proj = torch.nn.Linear(head_size * n_heads, n_embed)  # project from multiple heads to the single space\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        context = torch.cat([head(x, mask) for head in self.heads], dim=-1)\n",
    "\n",
    "        proj = self.proj(context)\n",
    "\n",
    "        out = self.dropout(proj)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, dropout, n_embed): # dropout=0.1, n_embed=3\n",
    "        super().__init__()\n",
    "\n",
    "        self.ffwd = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embed, 4 * n_embed),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(4 * n_embed, n_embed),\n",
    "            torch.nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ffwd(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BertLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Single layer of BERT transformer model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, dropout, n_embed): # n_heads=1, dropout=0.1, n_embed=3\n",
    "        super().__init__()\n",
    "\n",
    "        # unlike in the original paper, today in transformers it is more common to apply layer norm before other layers\n",
    "        # this idea is borrowed from Andrej Karpathy's series on transformers implementation\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(n_embed)\n",
    "        self.self_attention = BertSelfAttention(n_heads, dropout, n_embed)\n",
    "\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(n_embed)\n",
    "        self.feed_forward = FeedForward(dropout, n_embed)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.layer_norm1(x)\n",
    "        x = x + self.self_attention(x, mask)\n",
    "\n",
    "        x = self.layer_norm2(x)\n",
    "        out = x + self.feed_forward(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BertEncoder(torch.nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, dropout, n_embed): # n_layers=2, n_heads=1, dropout=0.1, n_embed=3\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([BertLayer(n_heads, dropout, n_embed) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BertPooler(torch.nn.Module):\n",
    "    def __init__(self, dropout, n_embed): # dropout=0.1, n_embed=3\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = torch.nn.Linear(in_features=n_embed, out_features=n_embed)\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.dense(x)\n",
    "        out = self.activation(pooled)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class NanoBERT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    NanoBERT is a almost an exact copy of a transformer decoder part described in the paper \"Attention is all you need\"\n",
    "    This is a base model that can be used for various purposes such as Masked Language Modelling, Classification,\n",
    "    Or any other kind of NLP tasks.\n",
    "    This implementation does not cover the Seq2Seq problem, but can be easily extended to that.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_layers, n_heads, dropout, n_embed, max_seq_len): # n_layers=2, n_heads=1, dropout=0.1, n_embed=4, max_seq_len = 16\n",
    "        \"\"\"\n",
    "\n",
    "        :param vocab_size: size of the vocabulary that tokenizer is using\n",
    "        :param n_layers: number of BERT layer in the model (default=2)\n",
    "        :param n_heads: number of heads in the MultiHeaded Self Attention Mechanism (default=1)\n",
    "        :param dropout: hidden dropout of the BERT model (default=0.1)\n",
    "        :param n_embed: hidden embeddings dimensionality (default=3)\n",
    "        :param max_seq_len: max length of the input sequence (default=16)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = ALBertEmbeddings(vocab_size, n_embed)\n",
    "#         self.embedding = BertEmbeddings(vocab_size, n_embed)\n",
    "        \n",
    "        self.position = PositionalEncoding(n_embed, max_seq_len)\n",
    "\n",
    "        self.encoder = BertEncoder(n_layers, n_heads, dropout, n_embed)\n",
    "\n",
    "        self.pooler = BertPooler(dropout, n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # attention masking for padded token\n",
    "        # (batch_size, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "\n",
    "        embeddings = self.embedding(x)\n",
    "        \n",
    "        position_embeddings = self.position(embeddings)\n",
    "\n",
    "        encoded = self.encoder(position_embeddings, mask)\n",
    "\n",
    "        pooled = self.pooler(encoded)\n",
    "        return pooled\n",
    "\n",
    "\n",
    "class BertMix(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This is a wrapper on the base NanoBERT that is used for classification task\n",
    "    One can use this as an example of how to extend and apply nano-BERT to similar custom tasks\n",
    "    This layer simply adds one additional dense layer for classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_layers=2, n_heads=1, dropout=0.1, n_embed=4, max_seq_len=16, n_classes=2): # n_layers=2, n_heads=1, dropout=0.1, n_embed=3, max_seq_len=16, n_classes=2\n",
    "        super().__init__()\n",
    "        self.nano_bert = NanoBERT(vocab_size, n_layers, n_heads, dropout, n_embed, max_seq_len)\n",
    "\n",
    "        self.classifier = torch.nn.Linear(in_features=n_embed, out_features=n_classes)\n",
    "        self.mlm = torch.nn.Linear(in_features=n_embed, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.nano_bert(input_ids)\n",
    "\n",
    "        r_cls = self.classifier(embeddings)\n",
    "        r_mlm = self.mlm(embeddings)\n",
    "        return r_cls, r_mlm\n",
    "    \n",
    "class BertMix3(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This is a wrapper on the base NanoBERT that is used for classification task\n",
    "    One can use this as an example of how to extend and apply nano-BERT to similar custom tasks\n",
    "    This layer simply adds one additional dense layer for classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_layers=2, n_heads=1, dropout=0.1, n_embed=4, max_seq_len=128, n_classes=2): # n_layers=2, n_heads=1, dropout=0.1, n_embed=3, max_seq_len=16, n_classes=2\n",
    "        super().__init__()\n",
    "        self.nano_bert = NanoBERT(vocab_size, n_layers, n_heads, dropout, n_embed, max_seq_len)\n",
    "\n",
    "        self.classifier = torch.nn.Linear(in_features=n_embed, out_features=n_classes)\n",
    "        self.mlm = torch.nn.Linear(in_features=n_embed, out_features=vocab_size)\n",
    "        self.nsp = torch.nn.Linear(in_features=n_embed, out_features=n_classes)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.nano_bert(input_ids)\n",
    "\n",
    "        r_cls = self.classifier(embeddings)\n",
    "        r_mlm = self.mlm(embeddings)\n",
    "        r_nsp = self.nsp(embeddings)\n",
    "        return r_cls, r_mlm, r_nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:39.504430Z",
     "start_time": "2023-10-17T21:26:39.490237Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LtAcsA8cXPK",
    "outputId": "afe80204-a636-4741-e051-928e3f66ac5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertMix3(\n",
       "  (nano_bert): NanoBERT(\n",
       "    (embedding): ALBertEmbeddings(\n",
       "      (hid_embeddings): Embedding(3665, 3)\n",
       "      (word_embeddings): Embedding(3, 4)\n",
       "    )\n",
       "    (position): PositionalEncoding(\n",
       "      (layer_norm): LayerNorm((4,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x BertLayer(\n",
       "          (layer_norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): BertSelfAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): BertAttentionHead(\n",
       "                (query): Linear(in_features=4, out_features=4, bias=True)\n",
       "                (key): Linear(in_features=4, out_features=4, bias=True)\n",
       "                (values): Linear(in_features=4, out_features=4, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (ffwd): Sequential(\n",
       "              (0): Linear(in_features=4, out_features=16, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=16, out_features=4, bias=True)\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=4, out_features=4, bias=True)\n",
       "      (activation): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (mlm): Linear(in_features=4, out_features=3665, bias=True)\n",
       "  (nsp): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = BertMix3(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    n_layers=3,\n",
    "    n_heads=1,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    n_classes=NUM_CLASS,\n",
    "    n_embed = 4\n",
    ").to(device)\n",
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:40.220320Z",
     "start_time": "2023-10-17T21:26:40.211284Z"
    },
    "id": "H64pDk8AcXPK"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:40.969226Z",
     "start_time": "2023-10-17T21:26:40.956953Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-Bhh81ScXPK",
    "outputId": "9529dd0e-ce4c-4324-b61f-a4128165d64f"
   },
   "outputs": [],
   "source": [
    "count_parameters(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.steps('train')\n",
    "dataloader.get_split('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def mask_text(cmt, vacab_size): # cmt : a comment\n",
    "    mps = []\n",
    "    mcmt = cmt.clone()\n",
    "    unique_elements, counts = torch.unique(cudata[i], return_counts=True)\n",
    "    n_sp = len(tokenizer.special_tokens) # number of special_tokens = 7\n",
    "    m_range = (MAX_SEQ_LEN - counts[0].item()) * 0.15 # range of masking indice\n",
    "#     print(m_range, counts[0].item())\n",
    "    while len(mps) < m_range:\n",
    "        temp = random.randint(0, MAX_SEQ_LEN - 1)\n",
    "        if mcmt[temp] > n_sp - 1:\n",
    "            mps.append(temp)\n",
    "            mcmt[temp] = 0 # set to mask\n",
    "    temp_m = random.sample(range(0, len(mps) - 1), 2 * int(m_range * 0.1 + 1)) # fetch some masked words to their original value\n",
    "    half = int(len(temp_m)/2)\n",
    "    tmps = torch.tensor(mps) # tensorlize mps to get location of masks to be changed\n",
    "    mcmt[tmps[temp_m[:half]]] = cmt[temp_m[:half]] # 10% percent original\n",
    "    mcmt[tmps[temp_m[half:]]] = torch.tensor([random.randint(n_sp, vacab_size - 1) for i in range(half)]).to(device) # 10% percent random vocab\n",
    "    return mps, mcmt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spit_cmt(cmt): # split  comment into sentences\n",
    "    bg = torch.where(cmt==5)[0] # begin at '[SOS]'\n",
    "    ed = torch.where(cmt==6)[0] # end at '.'\n",
    "    spit = []\n",
    "    n_st = len(bg) # number of next sentence prediction task in this comment\n",
    "    for i in range(n_st):\n",
    "        sts = cmt[bg[i].item():ed[i].item()+1]\n",
    "        spit.append(sts)\n",
    "    return spit, n_st\n",
    "def pad_nsp(nsp, max_seq_len): # add '[CLS]' and padding on given composed 2 sentences in GPU\n",
    "#     print('len nsp = ', len(nsp))\n",
    "    return torch.cat((torch.tensor([0]).to(device), nsp, torch.ones(max_seq_len - len(nsp) - 1).to(device)), 0)\n",
    "def nsp_gen(spit, n_st, max_seq_len): # generate nsp input      \n",
    "    nsp = torch.zeros((2 * (n_st - 1), max_seq_len)).to(device)\n",
    "    if n_st < 3:\n",
    "        nsp = torch.zeros((1, max_seq_len)).to(device)\n",
    "#     print(nsp[:10], n_st)\n",
    "    for id_i in range(n_st - 1):\n",
    "        nsp[2 * id_i] = pad_nsp(torch.cat((spit[id_i], spit[id_i + 1]), 0), max_seq_len) # a sentence followed by the next\n",
    "        if n_st > 2:\n",
    "            id_s = random.randint(0, n_st - 1) # selected id for nsp\n",
    "            while id_s == id_i + 1 or id_s == id_i: # excluded the next sentence\n",
    "                id_s = random.randint(0, n_st - 1)\n",
    "            nsp[2 * id_i + 1] = pad_nsp(torch.cat((spit[id_i], spit[id_s]), 0), max_seq_len) # a sentence not followed by the next\n",
    "    return nsp.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_pretrain = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'train_f1': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "history = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'train_f1': [],\n",
    "    'val_f1': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrain task, 25 min/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(bert.parameters(), lr=LEARNING_RATE)\n",
    "NUM_EPOCHS = 15 # epochs for pretrain task\n",
    "vacab_size = len(tokenizer.vocab)\n",
    "for i in range(NUM_EPOCHS):\n",
    "    print(f'Epoch: {i + 1}')\n",
    "    train_loss = 0.0\n",
    "\n",
    "    bert.train()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('train'), total=dataloader.steps('train'), position=0)):\n",
    "        loss = 0\n",
    "        cudata = batch['input_ids'].to(device) # put the dat in the whole batch to gpu\n",
    "        # MLM part\n",
    "        for i in range(len(cudata)): # BATCH_SIZE = len(cudata)\n",
    "            mps, mcmt = mask_text(cudata[i], vacab_size)\n",
    "            lmps = len(mps)\n",
    "            _, r_mlm, _ = bert(mcmt.unsqueeze(dim=0)) # (Batch, Seq_Len, len(vocab))\n",
    "            MCMT = r_mlm[0][mps, :] # fectch the predicted value from masked input\n",
    "            predm = cudata[i][mps].long() # fectch the real word correspond to the masked input\n",
    "            for i in range(lmps):\n",
    "                loss += F.cross_entropy(MCMT.to(device), predm.to(device)) / lmps\n",
    "        \n",
    "        # NSP part\n",
    "        for i in range(len(cudata)): \n",
    "            spit, n_st = spit_cmt(cudata[i]) # (Batch, Seq_Len, len(vocab)), number of sentences\n",
    "            if n_st > 1: # calculate NSP loss if a comment has more than 1 sentence\n",
    "                predn = torch.tensor([1, 0] * (n_st - 1)).to(device) # label of next/not next composed sentences\n",
    "                if n_st < 3:\n",
    "                    predn = torch.tensor([1]).to(device)\n",
    "                _, _, r_nsp = bert(nsp_gen(spit, n_st, MAX_SEQ_LEN))\n",
    "    #             print(n_st)\n",
    "    #             print(r_nsp[:, 0, :].shape, predn.shape)\n",
    "    #             print(r_nsp[:, 0, :], predn)\n",
    "                loss +=  F.cross_entropy(r_nsp[:, 0, :], predn)\n",
    "        print('train_loss:', '%.2f'%loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss += loss.item()\n",
    "    history_pretrain['train_losses'].append(train_loss / dataloader.steps(\"train\"))\n",
    "    val_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'imdb_pre_para.pth'# parameter for pretrain task\n",
    "torch.save(bert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning on text classification downstream task, 20s/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_F = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_F = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=BATCH_SIZE_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlLiYGbjcXPL",
    "outputId": "cfadd5cd-e236-4920-94ce-e3cffc3833db"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS_F = 200\n",
    "\n",
    "for i in range(NUM_EPOCHS_F):\n",
    "    print(f'Epoch: {i + 1}')\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    bert.train()\n",
    "    for step, batch in enumerate(tqdm(dataloader_F.get_split('train'), total=dataloader_F.steps('train'))):\n",
    "        r_cls, _, _ = bert(batch['input_ids'].to(device)) # (B, Seq_Len, 2)\n",
    "\n",
    "        probs = F.softmax(r_cls[:, 0, :], dim=-1).cpu()# fetch the result from the first word in the text as output\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        train_preds += pred.detach().tolist()\n",
    "        train_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        loss = F.cross_entropy(r_cls[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    bert.eval()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('val'), total=dataloader.steps('val'))):\n",
    "        r_cls, _, _ = bert(batch['input_ids'].to(device))\n",
    "\n",
    "        probs = F.softmax(r_cls[:, 0, :], dim=-1).cpu()\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        val_preds += pred.detach().tolist()\n",
    "        val_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        loss = F.cross_entropy(r_cls[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    history['train_losses'].append(train_loss)\n",
    "    history['val_losses'].append(val_loss)\n",
    "    history['train_acc'].append(accuracy_score(train_labels, train_preds))\n",
    "    history['val_acc'].append(accuracy_score(val_labels, val_preds))\n",
    "    history['train_f1'].append(f1_score(train_labels, train_preds))\n",
    "    history['val_f1'].append(f1_score(val_labels, val_preds))\n",
    "\n",
    "    print()\n",
    "    print(f'Train loss: {train_loss / dataloader.steps(\"train\")} | Val loss: {val_loss / dataloader.steps(\"val\")}')\n",
    "    print(f'Train acc: {accuracy_score(train_labels, train_preds)} | Val acc: {accuracy_score(val_labels, val_preds)}')\n",
    "    print(f'Train f1: {f1_score(train_labels, train_preds)} | Val f1: {f1_score(val_labels, val_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'imdb_tcl_para_BERT.pth'# parameter for text classification\n",
    "torch.save(bert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwxa3xgpN-TC"
   },
   "outputs": [],
   "source": [
    "def plot_results(history, do_val=True):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    x = list(range(0, len(history['train_losses'])))\n",
    "\n",
    "    # loss\n",
    "\n",
    "    ax.plot(x, history['train_losses'], label='train_loss')\n",
    "\n",
    "    if do_val:\n",
    "        ax.plot(x, history['val_losses'], label='val_loss')\n",
    "\n",
    "    plt.title('Train / Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # accuracy\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    ax.plot(x, history['train_acc'], label='train_acc')\n",
    "\n",
    "    if do_val:\n",
    "        ax.plot(x, history['val_acc'], label='val_acc')\n",
    "\n",
    "    plt.title('Train / Validation Accuracy')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # f1-score\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    ax.plot(x, history['train_f1'], label='train_f1')\n",
    "\n",
    "    if do_val:\n",
    "        ax.plot(x, history['val_f1'], label='val_f1')\n",
    "\n",
    "    plt.title('Train / Validation F1')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GWtJQPTtPRgf",
    "outputId": "624b70e4-b2dc-4878-f055-0f6754c819dd"
   },
   "outputs": [],
   "source": [
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FimwkDtWqzD",
    "outputId": "c2827b2a-9e07-4cc1-e198-940e0c1aa1d8"
   },
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "bert.eval()\n",
    "for step, batch in enumerate(tqdm(dataloader.get_split('test'), total=dataloader.steps('test'))):\n",
    "    logits = bert(batch['input_ids'].to(device))[0]\n",
    "\n",
    "    probs = F.softmax(logits[:, 0, :], dim=-1).cpu()\n",
    "    pred = torch.argmax(probs, dim=-1) # (B)\n",
    "    test_preds += pred.detach().tolist()\n",
    "    test_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "    loss = F.cross_entropy(logits[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "    test_loss += loss.item()\n",
    "\n",
    "print()\n",
    "print(f'Test loss: {test_loss / dataloader.steps(\"test\")}')\n",
    "print(f'Test acc: {accuracy_score(test_labels, test_preds)}')\n",
    "print(f'Test f1: {f1_score(test_labels, test_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlCBCY4xlUMj"
   },
   "source": [
    "# Interpreting and visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNon7i89dKoW"
   },
   "outputs": [],
   "source": [
    "def get_attention_scores(model, input_ids):\n",
    "    \"\"\"\n",
    "    This is just a wrapper to easily access attention heads of the last layer\n",
    "    \"\"\"\n",
    "\n",
    "    mask = (input_ids > 0).unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
    "\n",
    "    embed = model.nano_bert.embedding(input_ids)\n",
    "\n",
    "    # can be any layer, and we can also control what to do with output for each layer (aggregate, sum etc.)\n",
    "    layer = model.nano_bert.encoder.layers[-1]\n",
    "\n",
    "    x = layer.layer_norm1(embed)\n",
    "\n",
    "    B, seq_len, n_embed = x.shape\n",
    "\n",
    "    # if have more than 1 head, or interested in more than 1 head output just add aggregation here\n",
    "    head = layer.self_attention.heads[0]\n",
    "\n",
    "    # this is just a part of the single head that does all the computations (same code is present in AttentionHead)\n",
    "    q = head.query(x)\n",
    "    k = head.key(x)\n",
    "    v = head.values(x)\n",
    "\n",
    "    weights = (q @ k.transpose(-2, -1)) / math.sqrt(n_embed)  # (B, Seq_len, Seq_len)\n",
    "    weights = weights.masked_fill(mask == 0, -1e9)  # mask out not attended tokens\n",
    "\n",
    "    scores = F.softmax(weights, dim=-1)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cssoF5oSXC2V",
    "outputId": "7c10056c-56af-4db7-bd95-1d23bfceeb3e"
   },
   "outputs": [],
   "source": [
    "test_dataloader = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgyRO91Ng17v"
   },
   "outputs": [],
   "source": [
    "def plot_parallel(matrix, tokens):\n",
    "    # Set figsize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    input_len = len(tokens)\n",
    "\n",
    "    # Vertical lines\n",
    "    plt.axvline(x=1, color='black', linestyle='--', linewidth=1)\n",
    "    plt.axvline(x=5, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Add the A and B\n",
    "    plt.text(1, input_len + 1, 'A', fontsize=12, color='black', fontweight='bold')\n",
    "    plt.text(5, input_len + 1, 'B', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "    for i in range(input_len):\n",
    "        for j in range(input_len):\n",
    "            # Add the line to the plot\n",
    "            plt.plot([1, 5], [i, j], marker='o', label='token', color='blue', linewidth=5 * matrix[i][j])\n",
    "\n",
    "            plt.text(\n",
    "                1 - 0.18,  # x-axis position\n",
    "                i,  # y-axis position\n",
    "                tokens[i],  # Text\n",
    "                fontsize=8,  # Text size\n",
    "                color='black',  # Text color,\n",
    "            )\n",
    "\n",
    "            plt.text(\n",
    "                5 + 0.06,  # x-axis position\n",
    "                j,  # y-axis position\n",
    "                tokens[j],  # Text\n",
    "                fontsize=8,  # Text size\n",
    "                color='black',  # Text color\n",
    "            )\n",
    "        break\n",
    "\n",
    "    plt.title(f'Attention scores \\n\\n\\n')\n",
    "\n",
    "    plt.yticks([])  # Remove y-axis\n",
    "    plt.box(False)  # Remove the bounding box around plot\n",
    "    plt.show()  # Display the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRh1wc2Tf8Go",
    "outputId": "87c23765-e2cf-4b62-b741-553da6032f36"
   },
   "outputs": [],
   "source": [
    "# examples with less than 16 words are easier to visualize, so focus on them\n",
    "examples_ids = []\n",
    "for i, v in enumerate(test_dataloader.splits['test']):\n",
    "    if len(v) <= 16:\n",
    "        examples_ids.append(i)\n",
    "print(examples_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X2Mq57UVhD5F",
    "outputId": "a6da61db-cd06-4f91-a93b-c4ad625f6ec6"
   },
   "outputs": [],
   "source": [
    "for sample_index in examples_ids:\n",
    "    # extract example, decode to tokens and get the sequence length (ingoring padding)\n",
    "    test_tokenized_batch = test_dataloader.peek_index_tokenized(index=sample_index, split='test')\n",
    "    tokens = tokenizer.decode([t.item() for t in test_tokenized_batch['input_ids'][0] if (t != 0 and t.item() != 1)], ignore_special=False).split(' ')[:MAX_SEQ_LEN]\n",
    "    seq_len = len(tokens)\n",
    "\n",
    "    # calculate attention scores\n",
    "    att_matrix = get_attention_scores(bert, test_tokenized_batch['input_ids'].to(device))[0, :seq_len, :seq_len]\n",
    "\n",
    "    plot_parallel(att_matrix, tokens=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "311",
   "language": "python",
   "name": "311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
