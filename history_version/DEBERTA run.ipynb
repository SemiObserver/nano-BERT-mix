{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:21:56.394657Z",
     "start_time": "2023-10-17T21:21:52.683897Z"
    },
    "id": "EzQpePJ4cXPG"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nano_bert.tokenizer import WordTokenizer\n",
    "from nano_bert.dataload_imdb import encode_label, IMDBDataloader, mask_text, spit_cmt, nsp_gen\n",
    "\n",
    "from nano_bert.model import DEBertAMix3\n",
    "model = 'DEBERTA'\n",
    "torch.manual_seed(114514)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open('data/imdb_train.json') as f:\n",
    "    data = [json.loads(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = None\n",
    "with open('data/imdb_test.json') as f:\n",
    "    test_data = [json.loads(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawvocab = [] # whole vocab\n",
    "for d in tqdm(data): \n",
    "    rawvocab.append([w.lower() for w in d['text']]) # symbol like '.' is remained\n",
    "vocab = set() # vocab for words appear more than 2 times(minappear = 2)\n",
    "minappear = 2\n",
    "for v in tqdm(rawvocab):\n",
    "    if rawvocab.count(v) > minappear - 1:\n",
    "        vocab |= set(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:22:18.371680Z",
     "start_time": "2023-10-17T21:22:18.368560Z"
    },
    "id": "5b6HjLMEcXPJ"
   },
   "outputs": [],
   "source": [
    "NUM_CLASS = 2\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:23.407128Z",
     "start_time": "2023-10-17T21:26:23.403439Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CdJl4UEcXPJ",
    "outputId": "4ea3a032-0c97-4849-c825-28f189f7f643"
   },
   "outputs": [],
   "source": [
    "vocab.discard('.') # '.' is included in taken's vocab enumerate\n",
    "tokenizer = WordTokenizer(vocab=vocab, max_seq_len=MAX_SEQ_LEN)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:37.278112Z",
     "start_time": "2023-10-17T21:26:24.941273Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQMDcK01cXPK",
    "outputId": "b6b3b189-e1f3-46fd-d819-8ded067160ac",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataloader = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:39.504430Z",
     "start_time": "2023-10-17T21:26:39.490237Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LtAcsA8cXPK",
    "outputId": "afe80204-a636-4741-e051-928e3f66ac5a"
   },
   "outputs": [],
   "source": [
    "bert = BertMix3(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    n_layers=2,\n",
    "    n_heads=1,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    n_classes=NUM_CLASS,\n",
    "    n_embed = 4\n",
    ").to(device)\n",
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:40.220320Z",
     "start_time": "2023-10-17T21:26:40.211284Z"
    },
    "id": "H64pDk8AcXPK"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:40.969226Z",
     "start_time": "2023-10-17T21:26:40.956953Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-Bhh81ScXPK",
    "outputId": "9529dd0e-ce4c-4324-b61f-a4128165d64f"
   },
   "outputs": [],
   "source": [
    "count_parameters(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.steps('train')\n",
    "dataloader.get_split('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_pretrain = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'train_f1': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "history = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'train_f1': [],\n",
    "    'val_f1': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrain task, 25 min/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(bert.parameters(), lr=LEARNING_RATE)\n",
    "NUM_EPOCHS = 15 # epochs for pretrain task\n",
    "vacab_size = len(tokenizer.vocab)\n",
    "n_sp = len(tokenizer.special_tokens)\n",
    "for i in range(NUM_EPOCHS):\n",
    "    print(f'Epoch: {i + 1}')\n",
    "    train_loss = 0.0\n",
    "\n",
    "    bert.train()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('train'), total=dataloader.steps('train'), position=0)):\n",
    "        loss = 0\n",
    "        cudata = batch['input_ids'].to(device) # put the dat in the whole batch to gpu\n",
    "        # MLM part\n",
    "        for i in range(len(cudata)): # BATCH_SIZE = len(cudata)\n",
    "            mps, mcmt = mask_text(cudata[i], vacab_size, MAX_SEQ_LEN, n_sp)\n",
    "            lmps = len(mps)\n",
    "            _, r_mlm, _ = bert(mcmt.unsqueeze(dim=0)) # (Batch, Seq_Len, len(vocab))\n",
    "            MCMT = r_mlm[0][mps, :] # fectch the predicted value from masked input\n",
    "            predm = cudata[i][mps].long() # fectch the real word correspond to the masked input\n",
    "            for i in range(lmps):\n",
    "                loss += F.cross_entropy(MCMT.to(device), predm.to(device)) / lmps\n",
    "        \n",
    "        # NSP part\n",
    "        for i in range(len(cudata)): \n",
    "            spit, n_st = spit_cmt(cudata[i]) # (Batch, Seq_Len, len(vocab)), number of sentences\n",
    "            if n_st > 1: # calculate NSP loss if a comment has more than 1 sentence\n",
    "                predn = torch.tensor([1, 0] * (n_st - 1)).to(device) # label of next/not next composed sentences\n",
    "                if n_st < 3:\n",
    "                    predn = torch.tensor([1]).to(device)\n",
    "                _, _, r_nsp = bert(nsp_gen(spit, n_st, MAX_SEQ_LEN))\n",
    "#                 print(n_st)\n",
    "#                 print(r_nsp[:, 0, :].shape, predn.shape)\n",
    "#                 print(r_nsp[:, 0, :], predn)\n",
    "                loss +=  F.cross_entropy(r_nsp[:, 0, :], predn)\n",
    "#         print('train_loss:', '%.2f'%loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss += loss.item()\n",
    "    history_pretrain['train_losses'].append(train_loss / dataloader.steps(\"train\"))\n",
    "    print('train_loss:', '%.2f'%history_pretrain['train_losses'][-1])\n",
    "    val_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'para/'+ model + '_imdb_pre_para.pth'# parameter for pretrain task\n",
    "torch.save(bert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning on text classification downstream task, 20s/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_F = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_F = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=BATCH_SIZE_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlLiYGbjcXPL",
    "outputId": "cfadd5cd-e236-4920-94ce-e3cffc3833db"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS_F = 200\n",
    "\n",
    "for i in range(NUM_EPOCHS_F):\n",
    "    print(f'Epoch: {i + 1}')\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    bert.train()\n",
    "    for step, batch in enumerate(tqdm(dataloader_F.get_split('train'), total=dataloader_F.steps('train'))):\n",
    "        r_cls, _, _ = bert(batch['input_ids'].to(device)) # (B, Seq_Len, 2)\n",
    "\n",
    "        probs = F.softmax(r_cls[:, 0, :], dim=-1).cpu()# fetch the result from the first word in the text as output\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        train_preds += pred.detach().tolist()\n",
    "        train_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        loss = F.cross_entropy(r_cls[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    bert.eval()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('val'), total=dataloader.steps('val'))):\n",
    "        r_cls, _, _ = bert(batch['input_ids'].to(device))\n",
    "\n",
    "        probs = F.softmax(r_cls[:, 0, :], dim=-1).cpu()\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        val_preds += pred.detach().tolist()\n",
    "        val_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        loss = F.cross_entropy(r_cls[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    history['train_losses'].append(train_loss)\n",
    "    history['val_losses'].append(val_loss)\n",
    "    history['train_acc'].append(accuracy_score(train_labels, train_preds))\n",
    "    history['val_acc'].append(accuracy_score(val_labels, val_preds))\n",
    "    history['train_f1'].append(f1_score(train_labels, train_preds))\n",
    "    history['val_f1'].append(f1_score(val_labels, val_preds))\n",
    "\n",
    "    print()\n",
    "    print(f'Train loss: {train_loss / dataloader.steps(\"train\")} | Val loss: {val_loss / dataloader.steps(\"val\")}')\n",
    "    print(f'Train acc: {accuracy_score(train_labels, train_preds)} | Val acc: {accuracy_score(val_labels, val_preds)}')\n",
    "    print(f'Train f1: {f1_score(train_labels, train_preds)} | Val f1: {f1_score(val_labels, val_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'para/'+ model + 'imdb_tcl_para_BERT.pth'# parameter for text classification\n",
    "torch.save(bert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwxa3xgpN-TC"
   },
   "outputs": [],
   "source": [
    "def plot_results(history, do_val=True):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    x = list(range(0, len(history['train_losses'])))\n",
    "\n",
    "    # loss\n",
    "\n",
    "    ax.plot(x, history['train_losses'], label='train_loss')\n",
    "\n",
    "    if do_val:\n",
    "        ax.plot(x, history['val_losses'], label='val_loss')\n",
    "\n",
    "    plt.title(model + ' Train / Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig('pic/' + model + 'Loss')\n",
    "    plt.close()\n",
    "\n",
    "    # accuracy\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    ax.plot(x, history['train_acc'], label='train_acc')\n",
    "\n",
    "    if do_val:\n",
    "        ax.plot(x, history['val_acc'], label='val_acc')\n",
    "\n",
    "    plt.title(model + 'Train / Validation Accuracy')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig('pic/' + model + ' Accuracy')\n",
    "\n",
    "    # f1-score\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    ax.plot(x, history['train_f1'], label='train_f1')\n",
    "\n",
    "    if do_val:\n",
    "        ax.plot(x, history['val_f1'], label='val_f1')\n",
    "\n",
    "    plt.title('Train / Validation F1')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig('pic/' + model + 'Train / Validation Accuracy')\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GWtJQPTtPRgf",
    "outputId": "624b70e4-b2dc-4878-f055-0f6754c819dd"
   },
   "outputs": [],
   "source": [
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FimwkDtWqzD",
    "outputId": "c2827b2a-9e07-4cc1-e198-940e0c1aa1d8"
   },
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "bert.eval()\n",
    "for step, batch in enumerate(tqdm(dataloader.get_split('test'), total=dataloader.steps('test'))):\n",
    "    logits = bert(batch['input_ids'].to(device))[0]\n",
    "\n",
    "    probs = F.softmax(logits[:, 0, :], dim=-1).cpu()\n",
    "    pred = torch.argmax(probs, dim=-1) # (B)\n",
    "    test_preds += pred.detach().tolist()\n",
    "    test_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "    loss = F.cross_entropy(logits[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "    test_loss += loss.item()\n",
    "\n",
    "print()\n",
    "print(f'Test loss: {test_loss / dataloader.steps(\"test\")}')\n",
    "print(f'Test acc: {accuracy_score(test_labels, test_preds)}')\n",
    "print(f'Test f1: {f1_score(test_labels, test_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlCBCY4xlUMj"
   },
   "source": [
    "# Interpreting and visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNon7i89dKoW"
   },
   "outputs": [],
   "source": [
    "def get_attention_scores(model, input_ids):\n",
    "    \"\"\"\n",
    "    This is just a wrapper to easily access attention heads of the last layer\n",
    "    \"\"\"\n",
    "\n",
    "    mask = (input_ids > 0).unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
    "\n",
    "    embed = model.nano_bert.embedding(input_ids)\n",
    "\n",
    "    # can be any layer, and we can also control what to do with output for each layer (aggregate, sum etc.)\n",
    "    layer = model.nano_bert.encoder.layers[-1]\n",
    "\n",
    "    x = layer.layer_norm1(embed)\n",
    "\n",
    "    B, seq_len, n_embed = x.shape\n",
    "\n",
    "    # if have more than 1 head, or interested in more than 1 head output just add aggregation here\n",
    "    head = layer.self_attention.heads[0]\n",
    "\n",
    "    # this is just a part of the single head that does all the computations (same code is present in AttentionHead)\n",
    "    q = head.query(x)\n",
    "    k = head.key(x)\n",
    "    v = head.values(x)\n",
    "\n",
    "    weights = (q @ k.transpose(-2, -1)) / math.sqrt(n_embed)  # (B, Seq_len, Seq_len)\n",
    "    weights = weights.masked_fill(mask == 0, -1e9)  # mask out not attended tokens\n",
    "\n",
    "    scores = F.softmax(weights, dim=-1)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cssoF5oSXC2V",
    "outputId": "7c10056c-56af-4db7-bd95-1d23bfceeb3e"
   },
   "outputs": [],
   "source": [
    "test_dataloader = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgyRO91Ng17v"
   },
   "outputs": [],
   "source": [
    "def plot_parallel(matrix, tokens):\n",
    "    # Set figsize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    input_len = len(tokens)\n",
    "\n",
    "    # Vertical lines\n",
    "    plt.axvline(x=1, color='black', linestyle='--', linewidth=1)\n",
    "    plt.axvline(x=5, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Add the A and B\n",
    "    plt.text(1, input_len + 1, 'A', fontsize=12, color='black', fontweight='bold')\n",
    "    plt.text(5, input_len + 1, 'B', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "    for i in range(input_len):\n",
    "        for j in range(input_len):\n",
    "            # Add the line to the plot\n",
    "            plt.plot([1, 5], [i, j], marker='o', label='token', color='blue', linewidth=5 * matrix[i][j])\n",
    "\n",
    "            plt.text(\n",
    "                1 - 0.18,  # x-axis position\n",
    "                i,  # y-axis position\n",
    "                tokens[i],  # Text\n",
    "                fontsize=8,  # Text size\n",
    "                color='black',  # Text color,\n",
    "            )\n",
    "\n",
    "            plt.text(\n",
    "                5 + 0.06,  # x-axis position\n",
    "                j,  # y-axis position\n",
    "                tokens[j],  # Text\n",
    "                fontsize=8,  # Text size\n",
    "                color='black',  # Text color\n",
    "            )\n",
    "        break\n",
    "\n",
    "    plt.title(f'Attention scores \\n\\n\\n')\n",
    "\n",
    "    plt.yticks([])  # Remove y-axis\n",
    "    plt.box(False)  # Remove the bounding box around plot\n",
    "    plt.show()  # Display the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRh1wc2Tf8Go",
    "outputId": "87c23765-e2cf-4b62-b741-553da6032f36"
   },
   "outputs": [],
   "source": [
    "# examples with less than 16 words are easier to visualize, so focus on them\n",
    "examples_ids = []\n",
    "for i, v in enumerate(test_dataloader.splits['test']):\n",
    "    if len(v) <= 16:\n",
    "        examples_ids.append(i)\n",
    "print(examples_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X2Mq57UVhD5F",
    "outputId": "a6da61db-cd06-4f91-a93b-c4ad625f6ec6"
   },
   "outputs": [],
   "source": [
    "for sample_index in examples_ids:\n",
    "    # extract example, decode to tokens and get the sequence length (ingoring padding)\n",
    "    test_tokenized_batch = test_dataloader.peek_index_tokenized(index=sample_index, split='test')\n",
    "    tokens = tokenizer.decode([t.item() for t in test_tokenized_batch['input_ids'][0] if (t != 0 and t.item() != 1)], ignore_special=False).split(' ')[:MAX_SEQ_LEN]\n",
    "    seq_len = len(tokens)\n",
    "\n",
    "    # calculate attention scores\n",
    "    att_matrix = get_attention_scores(bert, test_tokenized_batch['input_ids'].to(device))[0, :seq_len, :seq_len]\n",
    "\n",
    "    plot_parallel(att_matrix, tokens=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "311",
   "language": "python",
   "name": "311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
