{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:21:56.394657Z",
     "start_time": "2023-10-17T21:21:52.683897Z"
    },
    "id": "EzQpePJ4cXPG"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nano_bert.model_ import BertMix3\n",
    "from nano_bert.tokenizer import WordTokenizer\n",
    "torch.manual_seed(114514)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open('data/imdb_train.json') as f:\n",
    "    data = [json.loads(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = None\n",
    "with open('data/imdb_test.json') as f:\n",
    "    test_data = [json.loads(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25000/25000 [00:00<00:00, 66957.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 25000/25000 [00:19<00:00, 1253.97it/s]\n"
     ]
    }
   ],
   "source": [
    "rawvocab = [] # whole vocab\n",
    "for d in tqdm(data): \n",
    "    rawvocab.append([w.lower() for w in d['text']]) # symbol like '.' is remained\n",
    "vocab = set() # vocab for words appear more than 2 times(minappear = 2)\n",
    "minappear = 2\n",
    "for v in tqdm(rawvocab):\n",
    "    if rawvocab.count(v) > minappear - 1:\n",
    "        vocab |= set(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:22:01.652524Z",
     "start_time": "2023-10-17T21:22:01.646787Z"
    },
    "id": "V1EiqscxcXPI"
   },
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "    if label == 'pos':\n",
    "        return 1\n",
    "    elif label == 'neg':\n",
    "        return 0\n",
    "    raise Exception(f'Unknown Label: {label}!')\n",
    "\n",
    "\n",
    "class IMDBDataloader:\n",
    "    def __init__(self, data, test_data, tokenizer, label_encoder, batch_size, val_frac=0.2):\n",
    "        train_data, val_data = train_test_split(data, shuffle=True, random_state=1, test_size=val_frac)\n",
    "\n",
    "        self.splits = {\n",
    "            'train': [d['text'] for d in train_data],\n",
    "            'test': [d['text'] for d in test_data],\n",
    "            'val': [d['text'] for d in val_data]\n",
    "        }\n",
    "\n",
    "        self.labels = {\n",
    "            'train': [d['label'] for d in train_data],\n",
    "            'test': [d['label'] for d in test_data],\n",
    "            'val': [d['label'] for d in val_data]\n",
    "        }\n",
    "\n",
    "        self.tokenized = {\n",
    "            'train': [tokenizer(record).unsqueeze(0) for record in\n",
    "                      tqdm(self.splits['train'], desc='Train Tokenization',position=0)], # divide different sentences in comments\n",
    "            'test': [tokenizer(record).unsqueeze(0) for record in tqdm(self.splits['test'], desc='Test Tokenization',position=0)],\n",
    "            'val': [tokenizer(record).unsqueeze(0) for record in tqdm(self.splits['val'], desc='Val Tokenization',position=0)],\n",
    "        }\n",
    "\n",
    "        self.encoded_labels = {\n",
    "            'train': [label_encoder(label) for label in tqdm(self.labels['train'], desc='Train Label Encoding',position=0)],\n",
    "            'test': [label_encoder(label) for label in tqdm(self.labels['test'], desc='Test Label Encoding',position=0)],\n",
    "            'val': [label_encoder(label) for label in tqdm(self.labels['val'], desc='Val Label Encoding',position=0)],\n",
    "        }\n",
    "\n",
    "        self.curr_batch = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.iterate_split = None\n",
    "\n",
    "    def peek(self, split):\n",
    "        return {\n",
    "            'input_ids': self.splits[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "            'label_ids': self.labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "        }\n",
    "\n",
    "    def take(self, split):\n",
    "        batch = self.splits[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        labels = self.labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        self.curr_batch += 1\n",
    "        return {\n",
    "            'input_ids': batch,\n",
    "            'label_ids': labels,\n",
    "        }\n",
    "\n",
    "    def peek_tokenized(self, split):\n",
    "        return {\n",
    "            'input_ids': torch.cat(\n",
    "                self.tokenized[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "                dim=0),\n",
    "            'label_ids': torch.tensor(\n",
    "                self.encoded_labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "                dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def peek_index_tokenized(self, index, split):\n",
    "        return {\n",
    "            'input_ids': torch.cat(\n",
    "                [self.tokenized[split][index]],\n",
    "                dim=0),\n",
    "            'label_ids': torch.tensor(\n",
    "                [self.encoded_labels[split][index]],\n",
    "                dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def peek_index(self, index, split):\n",
    "        return {\n",
    "            'input_ids': [self.splits[split][index]],\n",
    "            'label_ids': [self.labels[split][index]],\n",
    "        }\n",
    "\n",
    "    def take_tokenized(self, split):\n",
    "        batch = self.tokenized[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        labels = self.encoded_labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        self.curr_batch += 1\n",
    "        return {\n",
    "            'input_ids': torch.cat(batch, dim=0),\n",
    "            'label_ids': torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def get_split(self, split):\n",
    "        self.iterate_split = split\n",
    "        return self\n",
    "\n",
    "    def steps(self, split):\n",
    "        return len(self.tokenized[split]) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.reset()\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.batch_size * self.curr_batch < len(self.splits[self.iterate_split]):\n",
    "            return self.take_tokenized(self.iterate_split)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:22:18.371680Z",
     "start_time": "2023-10-17T21:22:18.368560Z"
    },
    "id": "5b6HjLMEcXPJ"
   },
   "outputs": [],
   "source": [
    "NUM_CLASS = 2\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:23.407128Z",
     "start_time": "2023-10-17T21:26:23.403439Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CdJl4UEcXPJ",
    "outputId": "4ea3a032-0c97-4849-c825-28f189f7f643"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer[vocab=3665,self.special_tokens=['[MSK]', '[PAD]', '[CLS]', '[SEP]', '[UNK]', '[SOS]', '.'],self.sep=' ',self.max_seq_len=128]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.discard('.') # '.' is included in taken's vocab enumerate\n",
    "tokenizer = WordTokenizer(vocab=vocab, max_seq_len=MAX_SEQ_LEN)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:37.278112Z",
     "start_time": "2023-10-17T21:26:24.941273Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQMDcK01cXPK",
    "outputId": "b6b3b189-e1f3-46fd-d819-8ded067160ac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Tokenization: 100%|██████████████████████████████████████████████████████| 20000/20000 [00:05<00:00, 3944.17it/s]\n",
      "Test Tokenization: 100%|███████████████████████████████████████████████████████| 25000/25000 [00:04<00:00, 5077.59it/s]\n",
      "Val Tokenization: 100%|██████████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 5070.44it/s]\n",
      "Train Label Encoding: 100%|█████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 1067483.81it/s]\n",
      "Test Label Encoding: 100%|███████████████████████████████████████████████████| 25000/25000 [00:00<00:00, 991111.36it/s]\n",
      "Val Label Encoding: 100%|██████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 701107.25it/s]\n"
     ]
    }
   ],
   "source": [
    "dataloader = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True, False]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,-1]])\n",
    "mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1451,  0.5471,  0.2341, -0.4797],\n",
       "         [-1.5129,  0.4042, -1.0225,  1.2885],\n",
       "         [ 0.1407,  0.3985, -0.5967,  0.9867],\n",
       "         [-1.2654,  2.0040,  0.6202,  0.7955],\n",
       "         [-2.2483, -0.5071,  1.5756, -0.9775]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([1,2,3,5,7]).unsqueeze(0)\n",
    "func = BertEmbeddings(128,4)\n",
    "func(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionEmbedding(torch.nn.Module): # for DEBERTA\n",
    "    def __init__(self, n_embed, max_seq_len):\n",
    "        super(AbsolutePositionEmbedding, self).__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, n_embed)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # Get the position IDs from the input IDs\n",
    "        position_ids = torch.arange(0, self.max_seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        # Get the position embeddings\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        return position_embeddings\n",
    "    \n",
    "class BertEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed): # n_embed = 3, max_seq_len = 16\n",
    "        super().__init__()\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, n_embed) # number of words is length of text, each words has length n_embed\n",
    "\n",
    "    def forward(self, x):\n",
    "        words_embeddings = self.word_embeddings(x)\n",
    "\n",
    "        return words_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.6926,  0.4761,  1.5245],\n",
      "          [ 3.4699, -0.0403,  2.7762],\n",
      "          [-1.0202,  0.1368,  1.3250],\n",
      "          [ 1.0911,  1.1973, -0.6746],\n",
      "          [-1.0955,  0.0870,  0.7239],\n",
      "          [-0.3229, -2.1459, -0.1938],\n",
      "          [-0.0115,  0.3485,  1.3729]],\n",
      "\n",
      "         [[ 1.6926,  0.4761,  1.5245],\n",
      "          [ 3.4699, -0.0403,  2.7762],\n",
      "          [-1.0202,  0.1368,  1.3250],\n",
      "          [ 1.0911,  1.1973, -0.6746],\n",
      "          [-1.0955,  0.0870,  0.7239],\n",
      "          [-0.3229, -2.1459, -0.1938],\n",
      "          [-0.0115,  0.3485,  1.3729]]]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[[ 0.1911,  0.1592, -1.1073],\n",
      "          [-1.2411,  1.7228, -1.4948],\n",
      "          [-1.1250,  0.5920, -0.1230],\n",
      "          [ 0.1156,  0.0412,  0.2202],\n",
      "          [-0.8667,  0.3806, -0.4301],\n",
      "          [ 0.1189,  1.9748,  0.1011],\n",
      "          [ 0.1189,  1.9748,  0.1011]],\n",
      "\n",
      "         [[ 0.1911,  0.1592, -1.1073],\n",
      "          [-1.2411,  1.7228, -1.4948],\n",
      "          [-1.1250,  0.5920, -0.1230],\n",
      "          [ 0.1156,  0.0412,  0.2202],\n",
      "          [-0.8667,  0.3806, -0.4301],\n",
      "          [ 0.1156,  0.0412,  0.2202],\n",
      "          [ 0.1156,  0.0412,  0.2202]]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([[[1,2,3,5,7,6,6], [1,2,3,5,7,5,5]]])\n",
    "fc = AbsolutePositionEmbedding(3, 7)\n",
    "fbe = BertEmbeddings(70, 3)\n",
    "print(fc(y))\n",
    "print(fbe(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 3 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fdeb \u001b[38;5;241m=\u001b[39m DEBertAEmbeddings(\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mfdeb\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[71], line 96\u001b[0m, in \u001b[0;36mDEBertAEmbeddings.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     92\u001b[0m words_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(x)\n\u001b[0;32m     94\u001b[0m abposits_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabposit_embeddings(x)\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabposits_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 3 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "fdeb = DEBertAEmbeddings(200, 7, 3)\n",
    "fdeb(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (7) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 7].  Tensor sizes: [1, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[0;32m     31\u001b[0m dbah \u001b[38;5;241m=\u001b[39m DEBertAAttentionHead(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mdbah\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[115], line 17\u001b[0m, in \u001b[0;36mDEBertAAttentionHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     16\u001b[0m ap \u001b[38;5;241m=\u001b[39m AbsolutePositionEmbedding(n_embed, seq_len)\n\u001b[1;32m---> 17\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[43map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(q\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     19\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(q)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[112], line 12\u001b[0m, in \u001b[0;36mAbsolutePositionEmbedding.forward\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Get the position IDs from the input IDs\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 12\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m \u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Get the position embeddings\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (7) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 7].  Tensor sizes: [1, 2]"
     ]
    }
   ],
   "source": [
    "class DEBertAAttentionHead(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, dropout, n_embed): # dropout = 0.1, n_embed = 3\n",
    "        super().__init__()\n",
    "\n",
    "        self.query = torch.nn.Linear(in_features=n_embed, out_features=head_size)\n",
    "        self.key = torch.nn.Linear(in_features=n_embed, out_features=head_size)\n",
    "        self.values = torch.nn.Linear(in_features=n_embed, out_features=head_size)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, Seq_len, N_embed\n",
    "        B, seq_len, n_embed = x.shape\n",
    "        print(x.shape)\n",
    "        ap = AbsolutePositionEmbedding(n_embed, seq_len)\n",
    "        q = ap(x[0])\n",
    "        print(q.shape)\n",
    "        q = self.query(q)\n",
    "        k = self.key(x)\n",
    "        v = self.values(x)\n",
    "\n",
    "        weights = (q @ k.transpose(-2, -1)) / math.sqrt(n_embed)  # (B, Seq_len, Seq_len)\n",
    "\n",
    "        scores = F.softmax(weights, dim=-1)\n",
    "        scores = self.dropout(scores)\n",
    "\n",
    "        context = scores @ v\n",
    "\n",
    "        return context\n",
    "dbah = DEBertAAttentionHead(1, 0.1, 3)\n",
    "dbah(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module): # for BERT and ALBERT\n",
    "    def __init__(self, n_embed, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create a matrix of shape (max_len, d_model) with positional encodings\n",
    "        pe = torch.zeros(max_seq_len, n_embed)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Div term represents the frequency of the sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, n_embed, 2).float() * (-torch.log(torch.tensor(10000.0)) / n_embed))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add a batch dimension for broadcasting\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        # Register pe as a buffer, which means it's not a parameter but should be part of the state\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(n_embed, eps=1e-12, elementwise_affine=True) # eps: added as sqrt(var + eps) to prevent zero denominator\n",
    "        self.dropout = torch.nn.Dropout(p=0.1, inplace=False) # inplace=False: do not replace the input by dropouted input\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input embeddings\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        embeddings = self.layer_norm(x)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class AbsolutePositionEmbedding(torch.nn.Module): # for DEBERTA\n",
    "    def __init__(self, n_embed, max_seq_len):\n",
    "        super(AbsolutePositionEmbedding, self).__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, n_embed)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # Get the position IDs from the input IDs\n",
    "        position_ids = torch.arange(0, self.max_seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        # Get the position embeddings\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        return position_embeddings\n",
    "    \n",
    "class BertEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed): # n_embed = 3, max_seq_len = 16\n",
    "        super().__init__()\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, n_embed) # number of words is length of text, each words has length n_embed\n",
    "\n",
    "    def forward(self, x):\n",
    "        words_embeddings = self.word_embeddings(x)\n",
    "\n",
    "        return words_embeddings\n",
    "    \n",
    "class ALBertEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed, n_hid = 3): # n_embed = 3\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_embeddings = torch.nn.Embedding(vocab_size, n_hid) # number of words is length of text, each words has length n_embed\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(n_hid, n_embed) # number of words is length of text, each words has length n_embed\n",
    "    def forward(self, x):\n",
    "        hid_embeddings = self.hid_embeddings(x)\n",
    "        \n",
    "        words_embeddings = self.word_embeddings(hid_embeddings)\n",
    "\n",
    "        return words_embeddings\n",
    "    \n",
    "class DEBertAEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, n_embed_word = 3, n_embed_p = 1): \n",
    "        super().__init__()\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, n_embed_word) # number of words is length of text, each words has length n_embed\n",
    "\n",
    "        self.abposit_embeddings = AbsolutePositionEmbedding(n_embed_p, max_seq_len)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        words_embeddings = self.word_embeddings(x)\n",
    "        \n",
    "        abposits_embeddings = self.abposit_embeddings(x)\n",
    "\n",
    "        return torch.cat((words_embeddings, abposits_embeddings), dim = 2)\n",
    "\n",
    "\n",
    "class BertAttentionHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head in MultiHeaded Self Attention layer.\n",
    "    The idea is identical to the original paper (\"Attention is all you need\"),\n",
    "    however instead of implementing multiple heads to be evaluated in parallel we matrix multiplication,\n",
    "    separated in a distinct class for easier and clearer interpretability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, dropout, n_embed): # dropout = 0.1, n_embed = 3\n",
    "        super().__init__()\n",
    "\n",
    "        self.query = torch.nn.Linear(in_features=n_embed, out_features=head_size)\n",
    "        self.key = torch.nn.Linear(in_features=n_embed, out_features=head_size)\n",
    "        self.values = torch.nn.Linear(in_features=n_embed, out_features=head_size)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # B, Seq_len, N_embed\n",
    "        B, seq_len, n_embed = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.values(x)\n",
    "\n",
    "        weights = (q @ k.transpose(-2, -1)) / math.sqrt(n_embed)  # (B, Seq_len, Seq_len)\n",
    "        weights = weights.masked_fill(mask == 0, -1e9)  # mask out not attended tokens\n",
    "\n",
    "        scores = F.softmax(weights, dim=-1)\n",
    "        scores = self.dropout(scores)\n",
    "\n",
    "        context = scores @ v\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class BertSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    MultiHeaded Self-Attention mechanism as described in \"Attention is all you need\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, dropout, n_embed): # , n_heads = 1, dropout = 0.1, n_embed = 3\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        n_heads = n_heads\n",
    "\n",
    "        self.heads = torch.nn.ModuleList([BertAttentionHead(head_size, dropout, n_embed) for _ in range(n_heads)])\n",
    "\n",
    "        self.proj = torch.nn.Linear(head_size * n_heads, n_embed)  # project from multiple heads to the single space\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        context = torch.cat([head(x, mask) for head in self.heads], dim=-1)\n",
    "\n",
    "        proj = self.proj(context)\n",
    "\n",
    "        out = self.dropout(proj)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, dropout, n_embed): # dropout=0.1, n_embed=3\n",
    "        super().__init__()\n",
    "\n",
    "        self.ffwd = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embed, 4 * n_embed),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(4 * n_embed, n_embed),\n",
    "            torch.nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ffwd(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BertLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Single layer of BERT transformer model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, dropout, n_embed): # n_heads=1, dropout=0.1, n_embed=3\n",
    "        super().__init__()\n",
    "\n",
    "        # unlike in the original paper, today in transformers it is more common to apply layer norm before other layers\n",
    "        # this idea is borrowed from Andrej Karpathy's series on transformers implementation\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(n_embed)\n",
    "        self.self_attention = BertSelfAttention(n_heads, dropout, n_embed)\n",
    "\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(n_embed)\n",
    "        self.feed_forward = FeedForward(dropout, n_embed)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.layer_norm1(x)\n",
    "        x = x + self.self_attention(x, mask)\n",
    "\n",
    "        x = self.layer_norm2(x)\n",
    "        out = x + self.feed_forward(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BertEncoder(torch.nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, dropout, n_embed): # n_layers=2, n_heads=1, dropout=0.1, n_embed=3\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([BertLayer(n_heads, dropout, n_embed) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BertPooler(torch.nn.Module):\n",
    "    def __init__(self, dropout, n_embed): # dropout=0.1, n_embed=3\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = torch.nn.Linear(in_features=n_embed, out_features=n_embed)\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.dense(x)\n",
    "        out = self.activation(pooled)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class NanoBERT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    NanoBERT is a almost an exact copy of a transformer decoder part described in the paper \"Attention is all you need\"\n",
    "    This is a base model that can be used for various purposes such as Masked Language Modelling, Classification,\n",
    "    Or any other kind of NLP tasks.\n",
    "    This implementation does not cover the Seq2Seq problem, but can be easily extended to that.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_layers, n_heads, dropout, n_embed, max_seq_len): # n_layers=2, n_heads=1, dropout=0.1, n_embed=4, max_seq_len = 16\n",
    "        \"\"\"\n",
    "\n",
    "        :param vocab_size: size of the vocabulary that tokenizer is using\n",
    "        :param n_layers: number of BERT layer in the model (default=2)\n",
    "        :param n_heads: number of heads in the MultiHeaded Self Attention Mechanism (default=1)\n",
    "        :param dropout: hidden dropout of the BERT model (default=0.1)\n",
    "        :param n_embed: hidden embeddings dimensionality (default=3)\n",
    "        :param max_seq_len: max length of the input sequence (default=16)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "#         self.embedding = BertEmbeddings(vocab_size, n_embed)\n",
    "#         self.embedding = ALBertEmbeddings(vocab_size, n_embed)\n",
    "        self.embedding = DEBertAEmbeddings(vocab_size, max_seq_len, n_embed-1, 1)\n",
    "        \n",
    "        self.position = PositionalEncoding(n_embed, max_seq_len)\n",
    "\n",
    "        self.encoder = BertEncoder(n_layers, n_heads, dropout, n_embed)\n",
    "\n",
    "        self.pooler = BertPooler(dropout, n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # attention masking for padded token\n",
    "        # (batch_size, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "\n",
    "        embeddings = self.embedding(x)\n",
    "        \n",
    "        position_embeddings = self.position(embeddings)\n",
    "\n",
    "        encoded = self.encoder(position_embeddings, mask)\n",
    "\n",
    "        pooled = self.pooler(encoded)\n",
    "        return pooled\n",
    "\n",
    "\n",
    "class BertMix(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This is a wrapper on the base NanoBERT that is used for classification task\n",
    "    One can use this as an example of how to extend and apply nano-BERT to similar custom tasks\n",
    "    This layer simply adds one additional dense layer for classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_layers=2, n_heads=1, dropout=0.1, n_embed=4, max_seq_len=16, n_classes=2): # n_layers=2, n_heads=1, dropout=0.1, n_embed=3, max_seq_len=16, n_classes=2\n",
    "        super().__init__()\n",
    "        self.nano_bert = NanoBERT(vocab_size, n_layers, n_heads, dropout, n_embed, max_seq_len)\n",
    "\n",
    "        self.classifier = torch.nn.Linear(in_features=n_embed, out_features=n_classes)\n",
    "        self.mlm = torch.nn.Linear(in_features=n_embed, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.nano_bert(input_ids)\n",
    "\n",
    "        r_cls = self.classifier(embeddings)\n",
    "        r_mlm = self.mlm(embeddings)\n",
    "        return r_cls, r_mlm\n",
    "    \n",
    "class BertMix3(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This is a wrapper on the base NanoBERT that is used for classification task\n",
    "    One can use this as an example of how to extend and apply nano-BERT to similar custom tasks\n",
    "    This layer simply adds one additional dense layer for classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_layers=2, n_heads=1, dropout=0.1, n_embed=4, max_seq_len=128, n_classes=2): # n_layers=2, n_heads=1, dropout=0.1, n_embed=3, max_seq_len=16, n_classes=2\n",
    "        super().__init__()\n",
    "        self.nano_bert = NanoBERT(vocab_size, n_layers, n_heads, dropout, n_embed, max_seq_len)\n",
    "\n",
    "        self.classifier = torch.nn.Linear(in_features=n_embed, out_features=n_classes)\n",
    "        self.mlm = torch.nn.Linear(in_features=n_embed, out_features=vocab_size)\n",
    "        self.nsp = torch.nn.Linear(in_features=n_embed, out_features=n_classes)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.nano_bert(input_ids)\n",
    "\n",
    "        r_cls = self.classifier(embeddings)\n",
    "        r_mlm = self.mlm(embeddings)\n",
    "        r_nsp = self.nsp(embeddings)\n",
    "        return r_cls, r_mlm, r_nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:39.504430Z",
     "start_time": "2023-10-17T21:26:39.490237Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LtAcsA8cXPK",
    "outputId": "afe80204-a636-4741-e051-928e3f66ac5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertMix3(\n",
       "  (nano_bert): NanoBERT(\n",
       "    (embedding): DEBertAEmbeddings(\n",
       "      (word_embeddings): Embedding(3665, 3)\n",
       "      (abposit_embeddings): AbsolutePositionEmbedding(\n",
       "        (position_embeddings): Embedding(128, 1)\n",
       "      )\n",
       "    )\n",
       "    (position): PositionalEncoding(\n",
       "      (layer_norm): LayerNorm((4,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (layer_norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): BertSelfAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): BertAttentionHead(\n",
       "                (query): Linear(in_features=4, out_features=4, bias=True)\n",
       "                (key): Linear(in_features=4, out_features=4, bias=True)\n",
       "                (values): Linear(in_features=4, out_features=4, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (ffwd): Sequential(\n",
       "              (0): Linear(in_features=4, out_features=16, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=16, out_features=4, bias=True)\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=4, out_features=4, bias=True)\n",
       "      (activation): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (mlm): Linear(in_features=4, out_features=3665, bias=True)\n",
       "  (nsp): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = BertMix3(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    n_layers=2,\n",
    "    n_heads=1,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    n_classes=NUM_CLASS,\n",
    "    n_embed = 4\n",
    ").to(device)\n",
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:40.220320Z",
     "start_time": "2023-10-17T21:26:40.211284Z"
    },
    "id": "H64pDk8AcXPK"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:40.969226Z",
     "start_time": "2023-10-17T21:26:40.956953Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-Bhh81ScXPK",
    "outputId": "9529dd0e-ce4c-4324-b61f-a4128165d64f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29984"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.steps('train')\n",
    "dataloader.get_split('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def mask_text(cmt, vacab_size): # cmt : a comment\n",
    "    mps = []\n",
    "    mcmt = cmt.clone()\n",
    "    unique_elements, counts = torch.unique(cudata[i], return_counts=True)\n",
    "    n_sp = len(tokenizer.special_tokens) # number of special_tokens = 7\n",
    "    m_range = (MAX_SEQ_LEN - counts[0].item()) * 0.15 # range of masking indice\n",
    "#     print(m_range, counts[0].item())\n",
    "    while len(mps) < m_range:\n",
    "        temp = random.randint(0, MAX_SEQ_LEN - 1)\n",
    "        if mcmt[temp] > n_sp - 1:\n",
    "            mps.append(temp)\n",
    "            mcmt[temp] = 0 # set to mask\n",
    "    temp_m = random.sample(range(0, len(mps) - 1), 2 * int(m_range * 0.1 + 1)) # fetch some masked words to their original value\n",
    "    half = int(len(temp_m)/2)\n",
    "    tmps = torch.tensor(mps) # tensorlize mps to get location of masks to be changed\n",
    "    mcmt[tmps[temp_m[:half]]] = cmt[temp_m[:half]] # 10% percent original\n",
    "    mcmt[tmps[temp_m[half:]]] = torch.tensor([random.randint(n_sp, vacab_size - 1) for i in range(half)]).to(device) # 10% percent random vocab\n",
    "    return mps, mcmt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spit_cmt(cmt): # split  comment into sentences\n",
    "    bg = torch.where(cmt==5)[0] # begin at '[SOS]'\n",
    "    ed = torch.where(cmt==6)[0] # end at '.'\n",
    "    spit = []\n",
    "    n_st = len(bg) # number of next sentence prediction task in this comment\n",
    "    for i in range(n_st):\n",
    "        sts = cmt[bg[i].item():ed[i].item()+1]\n",
    "        spit.append(sts)\n",
    "    return spit, n_st\n",
    "def pad_nsp(nsp, max_seq_len): # add '[CLS]' and padding on given composed 2 sentences in GPU\n",
    "#     print('len nsp = ', len(nsp))\n",
    "    return torch.cat((torch.tensor([0]).to(device), nsp, torch.ones(max_seq_len - len(nsp) - 1).to(device)), 0)\n",
    "def nsp_gen(spit, n_st, max_seq_len): # generate nsp input      \n",
    "    nsp = torch.zeros((2 * (n_st - 1), max_seq_len)).to(device)\n",
    "    if n_st < 3:\n",
    "        nsp = torch.zeros((1, max_seq_len)).to(device)\n",
    "#     print(nsp[:10], n_st)\n",
    "    for id_i in range(n_st - 1):\n",
    "        nsp[2 * id_i] = pad_nsp(torch.cat((spit[id_i], spit[id_i + 1]), 0), max_seq_len) # a sentence followed by the next\n",
    "        if n_st > 2:\n",
    "            id_s = random.randint(0, n_st - 1) # selected id for nsp\n",
    "            while id_s == id_i + 1 or id_s == id_i: # excluded the next sentence\n",
    "                id_s = random.randint(0, n_st - 1)\n",
    "            nsp[2 * id_i + 1] = pad_nsp(torch.cat((spit[id_i], spit[id_s]), 0), max_seq_len) # a sentence not followed by the next\n",
    "    return nsp.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_pretrain = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'train_f1': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "history = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'train_f1': [],\n",
    "    'val_f1': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrain task, 25 min/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(bert.parameters(), lr=LEARNING_RATE)\n",
    "NUM_EPOCHS = 15 # epochs for pretrain task\n",
    "vacab_size = len(tokenizer.vocab)\n",
    "for i in range(NUM_EPOCHS):\n",
    "    print(f'Epoch: {i + 1}')\n",
    "    train_loss = 0.0\n",
    "\n",
    "    bert.train()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('train'), total=dataloader.steps('train'), position=0)):\n",
    "        loss = 0\n",
    "        cudata = batch['input_ids'].to(device) # put the dat in the whole batch to gpu\n",
    "        # MLM part\n",
    "        for i in range(len(cudata)): # BATCH_SIZE = len(cudata)\n",
    "            mps, mcmt = mask_text(cudata[i], vacab_size)\n",
    "            lmps = len(mps)\n",
    "            _, r_mlm, _ = bert(mcmt.unsqueeze(dim=0)) # (Batch, Seq_Len, len(vocab))\n",
    "            MCMT = r_mlm[0][mps, :] # fectch the predicted value from masked input\n",
    "            predm = cudata[i][mps].long() # fectch the real word correspond to the masked input\n",
    "            for i in range(lmps):\n",
    "                loss += F.cross_entropy(MCMT.to(device), predm.to(device)) / lmps\n",
    "        \n",
    "        # NSP part\n",
    "        for i in range(len(cudata)): \n",
    "            spit, n_st = spit_cmt(cudata[i]) # (Batch, Seq_Len, len(vocab)), number of sentences\n",
    "            if n_st > 1: # calculate NSP loss if a comment has more than 1 sentence\n",
    "                predn = torch.tensor([1, 0] * (n_st - 1)).to(device) # label of next/not next composed sentences\n",
    "                if n_st < 3:\n",
    "                    predn = torch.tensor([1]).to(device)\n",
    "                _, _, r_nsp = bert(nsp_gen(spit, n_st, MAX_SEQ_LEN))\n",
    "    #             print(n_st)\n",
    "    #             print(r_nsp[:, 0, :].shape, predn.shape)\n",
    "    #             print(r_nsp[:, 0, :], predn)\n",
    "                loss +=  F.cross_entropy(r_nsp[:, 0, :], predn)\n",
    "        print('train_loss:', '%.2f'%loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss += loss.item()\n",
    "    history_pretrain['train_losses'].append(train_loss / dataloader.steps(\"train\"))\n",
    "    val_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'imdb_pre_para.pth'# parameter for pretrain task\n",
    "torch.save(bert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning on text classification downstream task, 20s/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_F = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_F = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=BATCH_SIZE_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlLiYGbjcXPL",
    "outputId": "cfadd5cd-e236-4920-94ce-e3cffc3833db"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS_F = 200\n",
    "\n",
    "for i in range(NUM_EPOCHS_F):\n",
    "    print(f'Epoch: {i + 1}')\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    bert.train()\n",
    "    for step, batch in enumerate(tqdm(dataloader_F.get_split('train'), total=dataloader_F.steps('train'))):\n",
    "        r_cls, _, _ = bert(batch['input_ids'].to(device)) # (B, Seq_Len, 2)\n",
    "\n",
    "        probs = F.softmax(r_cls[:, 0, :], dim=-1).cpu()# fetch the result from the first word in the text as output\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        train_preds += pred.detach().tolist()\n",
    "        train_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        loss = F.cross_entropy(r_cls[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    bert.eval()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('val'), total=dataloader.steps('val'))):\n",
    "        r_cls, _, _ = bert(batch['input_ids'].to(device))\n",
    "\n",
    "        probs = F.softmax(r_cls[:, 0, :], dim=-1).cpu()\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        val_preds += pred.detach().tolist()\n",
    "        val_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        loss = F.cross_entropy(r_cls[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    history['train_losses'].append(train_loss)\n",
    "    history['val_losses'].append(val_loss)\n",
    "    history['train_acc'].append(accuracy_score(train_labels, train_preds))\n",
    "    history['val_acc'].append(accuracy_score(val_labels, val_preds))\n",
    "    history['train_f1'].append(f1_score(train_labels, train_preds))\n",
    "    history['val_f1'].append(f1_score(val_labels, val_preds))\n",
    "\n",
    "    print()\n",
    "    print(f'Train loss: {train_loss / dataloader.steps(\"train\")} | Val loss: {val_loss / dataloader.steps(\"val\")}')\n",
    "    print(f'Train acc: {accuracy_score(train_labels, train_preds)} | Val acc: {accuracy_score(val_labels, val_preds)}')\n",
    "    print(f'Train f1: {f1_score(train_labels, train_preds)} | Val f1: {f1_score(val_labels, val_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'imdb_tcl_para_BERT.pth'# parameter for text classification\n",
    "torch.save(bert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwxa3xgpN-TC"
   },
   "outputs": [],
   "source": [
    "def plot_results(history, do_val=True):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    x = list(range(0, len(history['train_losses'])))\n",
    "\n",
    "    # loss\n",
    "\n",
    "    ax.plot(x, history['train_losses'], label='train_loss')\n",
    "\n",
    "    if do_val:\n",
    "        ax.plot(x, history['val_losses'], label='val_loss')\n",
    "\n",
    "    plt.title('Train / Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # accuracy\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    ax.plot(x, history['train_acc'], label='train_acc')\n",
    "\n",
    "    if do_val:\n",
    "        ax.plot(x, history['val_acc'], label='val_acc')\n",
    "\n",
    "    plt.title('Train / Validation Accuracy')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # f1-score\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    ax.plot(x, history['train_f1'], label='train_f1')\n",
    "\n",
    "    if do_val:\n",
    "        ax.plot(x, history['val_f1'], label='val_f1')\n",
    "\n",
    "    plt.title('Train / Validation F1')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GWtJQPTtPRgf",
    "outputId": "624b70e4-b2dc-4878-f055-0f6754c819dd"
   },
   "outputs": [],
   "source": [
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FimwkDtWqzD",
    "outputId": "c2827b2a-9e07-4cc1-e198-940e0c1aa1d8"
   },
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "bert.eval()\n",
    "for step, batch in enumerate(tqdm(dataloader.get_split('test'), total=dataloader.steps('test'))):\n",
    "    logits = bert(batch['input_ids'].to(device))[0]\n",
    "\n",
    "    probs = F.softmax(logits[:, 0, :], dim=-1).cpu()\n",
    "    pred = torch.argmax(probs, dim=-1) # (B)\n",
    "    test_preds += pred.detach().tolist()\n",
    "    test_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "    loss = F.cross_entropy(logits[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "    test_loss += loss.item()\n",
    "\n",
    "print()\n",
    "print(f'Test loss: {test_loss / dataloader.steps(\"test\")}')\n",
    "print(f'Test acc: {accuracy_score(test_labels, test_preds)}')\n",
    "print(f'Test f1: {f1_score(test_labels, test_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlCBCY4xlUMj"
   },
   "source": [
    "# Interpreting and visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNon7i89dKoW"
   },
   "outputs": [],
   "source": [
    "def get_attention_scores(model, input_ids):\n",
    "    \"\"\"\n",
    "    This is just a wrapper to easily access attention heads of the last layer\n",
    "    \"\"\"\n",
    "\n",
    "    mask = (input_ids > 0).unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
    "\n",
    "    embed = model.nano_bert.embedding(input_ids)\n",
    "\n",
    "    # can be any layer, and we can also control what to do with output for each layer (aggregate, sum etc.)\n",
    "    layer = model.nano_bert.encoder.layers[-1]\n",
    "\n",
    "    x = layer.layer_norm1(embed)\n",
    "\n",
    "    B, seq_len, n_embed = x.shape\n",
    "\n",
    "    # if have more than 1 head, or interested in more than 1 head output just add aggregation here\n",
    "    head = layer.self_attention.heads[0]\n",
    "\n",
    "    # this is just a part of the single head that does all the computations (same code is present in AttentionHead)\n",
    "    q = head.query(x)\n",
    "    k = head.key(x)\n",
    "    v = head.values(x)\n",
    "\n",
    "    weights = (q @ k.transpose(-2, -1)) / math.sqrt(n_embed)  # (B, Seq_len, Seq_len)\n",
    "    weights = weights.masked_fill(mask == 0, -1e9)  # mask out not attended tokens\n",
    "\n",
    "    scores = F.softmax(weights, dim=-1)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cssoF5oSXC2V",
    "outputId": "7c10056c-56af-4db7-bd95-1d23bfceeb3e"
   },
   "outputs": [],
   "source": [
    "test_dataloader = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgyRO91Ng17v"
   },
   "outputs": [],
   "source": [
    "def plot_parallel(matrix, tokens):\n",
    "    # Set figsize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    input_len = len(tokens)\n",
    "\n",
    "    # Vertical lines\n",
    "    plt.axvline(x=1, color='black', linestyle='--', linewidth=1)\n",
    "    plt.axvline(x=5, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Add the A and B\n",
    "    plt.text(1, input_len + 1, 'A', fontsize=12, color='black', fontweight='bold')\n",
    "    plt.text(5, input_len + 1, 'B', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "    for i in range(input_len):\n",
    "        for j in range(input_len):\n",
    "            # Add the line to the plot\n",
    "            plt.plot([1, 5], [i, j], marker='o', label='token', color='blue', linewidth=5 * matrix[i][j])\n",
    "\n",
    "            plt.text(\n",
    "                1 - 0.18,  # x-axis position\n",
    "                i,  # y-axis position\n",
    "                tokens[i],  # Text\n",
    "                fontsize=8,  # Text size\n",
    "                color='black',  # Text color,\n",
    "            )\n",
    "\n",
    "            plt.text(\n",
    "                5 + 0.06,  # x-axis position\n",
    "                j,  # y-axis position\n",
    "                tokens[j],  # Text\n",
    "                fontsize=8,  # Text size\n",
    "                color='black',  # Text color\n",
    "            )\n",
    "        break\n",
    "\n",
    "    plt.title(f'Attention scores \\n\\n\\n')\n",
    "\n",
    "    plt.yticks([])  # Remove y-axis\n",
    "    plt.box(False)  # Remove the bounding box around plot\n",
    "    plt.show()  # Display the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRh1wc2Tf8Go",
    "outputId": "87c23765-e2cf-4b62-b741-553da6032f36"
   },
   "outputs": [],
   "source": [
    "# examples with less than 16 words are easier to visualize, so focus on them\n",
    "examples_ids = []\n",
    "for i, v in enumerate(test_dataloader.splits['test']):\n",
    "    if len(v) <= 16:\n",
    "        examples_ids.append(i)\n",
    "print(examples_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X2Mq57UVhD5F",
    "outputId": "a6da61db-cd06-4f91-a93b-c4ad625f6ec6"
   },
   "outputs": [],
   "source": [
    "for sample_index in examples_ids:\n",
    "    # extract example, decode to tokens and get the sequence length (ingoring padding)\n",
    "    test_tokenized_batch = test_dataloader.peek_index_tokenized(index=sample_index, split='test')\n",
    "    tokens = tokenizer.decode([t.item() for t in test_tokenized_batch['input_ids'][0] if (t != 0 and t.item() != 1)], ignore_special=False).split(' ')[:MAX_SEQ_LEN]\n",
    "    seq_len = len(tokens)\n",
    "\n",
    "    # calculate attention scores\n",
    "    att_matrix = get_attention_scores(bert, test_tokenized_batch['input_ids'].to(device))[0, :seq_len, :seq_len]\n",
    "\n",
    "    plot_parallel(att_matrix, tokens=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "311",
   "language": "python",
   "name": "311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
