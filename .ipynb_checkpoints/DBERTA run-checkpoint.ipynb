{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:21:56.394657Z",
     "start_time": "2023-10-17T21:21:52.683897Z"
    },
    "id": "EzQpePJ4cXPG"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nano_bert.tokenizer import WordTokenizer\n",
    "from nano_bert.dataload_imdb import encode_label, IMDBDataloader, mask_text, spit_cmt, nsp_gen, plot_results\n",
    "\n",
    "from nano_bert.model import DEBertAMix3\n",
    "model = 'DEBERTA'\n",
    "torch.manual_seed(114514)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open('data/imdb_train.json') as f:\n",
    "    data = [json.loads(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = None\n",
    "with open('data/imdb_test.json') as f:\n",
    "    test_data = [json.loads(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25000/25000 [00:00<00:00, 63789.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 25000/25000 [00:04<00:00, 5742.36it/s]\n"
     ]
    }
   ],
   "source": [
    "rawvocab = [] # whole vocab\n",
    "for d in tqdm(data): \n",
    "    rawvocab.append([w.lower() for w in d['text']]) # symbol like '.' is remained\n",
    "vocab = set() # vocab for words appear more than 2 times(minappear = 2)\n",
    "minappear = 2\n",
    "for v in tqdm(rawvocab):\n",
    "    if rawvocab.count(v) > minappear - 1:\n",
    "        vocab |= set(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:22:18.371680Z",
     "start_time": "2023-10-17T21:22:18.368560Z"
    },
    "id": "5b6HjLMEcXPJ"
   },
   "outputs": [],
   "source": [
    "NUM_CLASS = 2\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:23.407128Z",
     "start_time": "2023-10-17T21:26:23.403439Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CdJl4UEcXPJ",
    "outputId": "4ea3a032-0c97-4849-c825-28f189f7f643"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer[vocab=3665,self.special_tokens=['[MSK]', '[PAD]', '[CLS]', '[SEP]', '[UNK]', '[SOS]', '.'],self.sep=' ',self.max_seq_len=128]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.discard('.') # '.' is included in taken's vocab enumerate\n",
    "tokenizer = WordTokenizer(vocab=vocab, max_seq_len=MAX_SEQ_LEN)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:37.278112Z",
     "start_time": "2023-10-17T21:26:24.941273Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQMDcK01cXPK",
    "outputId": "b6b3b189-e1f3-46fd-d819-8ded067160ac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Tokenization: 100%|██████████████████████████████████████████████████████| 20000/20000 [00:03<00:00, 5032.89it/s]\n",
      "Test Tokenization: 100%|███████████████████████████████████████████████████████| 25000/25000 [00:06<00:00, 4141.27it/s]\n",
      "Val Tokenization: 100%|██████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4874.73it/s]\n",
      "Train Label Encoding: 100%|█████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 1158503.50it/s]\n",
      "Test Label Encoding: 100%|██████████████████████████████████████████████████| 25000/25000 [00:00<00:00, 1148847.40it/s]\n",
      "Val Label Encoding: 100%|██████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 697539.33it/s]\n"
     ]
    }
   ],
   "source": [
    "dataloader = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:39.504430Z",
     "start_time": "2023-10-17T21:26:39.490237Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LtAcsA8cXPK",
    "outputId": "afe80204-a636-4741-e051-928e3f66ac5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEBertAMix3(\n",
       "  (nano_bert): NanoDEBERTA(\n",
       "    (embedding): DEBertAEmbeddings(\n",
       "      (word_embeddings): Embedding(3665, 3)\n",
       "      (abposit_embeddings): AbsolutePositionEmbedding(\n",
       "        (position_embeddings): Embedding(128, 1)\n",
       "      )\n",
       "    )\n",
       "    (embedding_I): AbsolutePositionEmbedding(\n",
       "      (position_embeddings): Embedding(128, 4)\n",
       "    )\n",
       "    (encoder): DEBertAEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x DEBertALayer(\n",
       "          (layer_norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): DEBertASelfAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): DEBertAAttentionHead(\n",
       "                (query): Linear(in_features=4, out_features=4, bias=True)\n",
       "                (key): Linear(in_features=4, out_features=4, bias=True)\n",
       "                (values): Linear(in_features=4, out_features=4, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (ffwd): Sequential(\n",
       "              (0): Linear(in_features=4, out_features=16, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=16, out_features=4, bias=True)\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=4, out_features=4, bias=True)\n",
       "      (activation): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (mlm): Linear(in_features=4, out_features=3665, bias=True)\n",
       "  (nsp): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = DEBertAMix3(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    n_layers=2,\n",
    "    n_heads=1,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    n_classes=NUM_CLASS,\n",
    "    n_embed = 4\n",
    ").to(device)\n",
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:40.220320Z",
     "start_time": "2023-10-17T21:26:40.211284Z"
    },
    "id": "H64pDk8AcXPK"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T21:26:40.969226Z",
     "start_time": "2023-10-17T21:26:40.956953Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-Bhh81ScXPK",
    "outputId": "9529dd0e-ce4c-4324-b61f-a4128165d64f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30732"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nano_bert.dataload_imdb.IMDBDataloader at 0x166fa7b6b90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.steps('train')\n",
    "dataloader.get_split('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_pretrain = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'train_f1': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "history = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'train_f1': [],\n",
    "    'val_f1': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrain task, 25 min/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                 | 7/625 [00:27<40:54,  3.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m mps, mcmt \u001b[38;5;241m=\u001b[39m mask_text(cudata[i], vacab_size, MAX_SEQ_LEN, n_sp)\n\u001b[0;32m     16\u001b[0m lmps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(mps)\n\u001b[1;32m---> 17\u001b[0m _, r_mlm, _ \u001b[38;5;241m=\u001b[39m \u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmcmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (Batch, Seq_Len, len(vocab))\u001b[39;00m\n\u001b[0;32m     18\u001b[0m MCMT \u001b[38;5;241m=\u001b[39m r_mlm[\u001b[38;5;241m0\u001b[39m][mps, :] \u001b[38;5;66;03m# fectch the predicted value from masked input\u001b[39;00m\n\u001b[0;32m     19\u001b[0m predm \u001b[38;5;241m=\u001b[39m cudata[i][mps]\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;66;03m# fectch the real word correspond to the masked input\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\jupyter\\nano-BERT-mix\\nano_bert\\model.py:453\u001b[0m, in \u001b[0;36mDEBertAMix3.forward\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids):\n\u001b[1;32m--> 453\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnano_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     r_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(embeddings)\n\u001b[0;32m    456\u001b[0m     r_mlm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlm(embeddings)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\jupyter\\nano-BERT-mix\\nano_bert\\model.py:433\u001b[0m, in \u001b[0;36mNanoDEBERTA.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    431\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x) \n\u001b[0;32m    432\u001b[0m embeddings_I \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_I(x)\n\u001b[1;32m--> 433\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_I\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m pooled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoded)\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pooled\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\jupyter\\nano-BERT-mix\\nano_bert\\model.py:412\u001b[0m, in \u001b[0;36mDEBertAEncoder.forward\u001b[1;34m(self, I, x, mask)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 412\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, x, mask)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\jupyter\\nano-BERT-mix\\nano_bert\\model.py:395\u001b[0m, in \u001b[0;36mDEBertALayer.forward\u001b[1;34m(self, I, x, mask)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, I, x, mask):\n\u001b[0;32m    394\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(x)\n\u001b[1;32m--> 395\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm2(x)\n\u001b[0;32m    398\u001b[0m     out \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(x)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\jupyter\\nano-BERT-mix\\nano_bert\\model.py:370\u001b[0m, in \u001b[0;36mDEBertASelfAttention.forward\u001b[1;34m(self, I, x, mask)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, I, x, mask):\n\u001b[1;32m--> 370\u001b[0m     context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    372\u001b[0m     proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(context)\n\u001b[0;32m    374\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(proj)\n",
      "File \u001b[1;32mF:\\jupyter\\nano-BERT-mix\\nano_bert\\model.py:370\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, I, x, mask):\n\u001b[1;32m--> 370\u001b[0m     context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    372\u001b[0m     proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(context)\n\u001b[0;32m    374\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(proj)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\jupyter\\nano-BERT-mix\\nano_bert\\model.py:337\u001b[0m, in \u001b[0;36mDEBertAAttentionHead.forward\u001b[1;34m(self, I, x, mask)\u001b[0m\n\u001b[0;32m    334\u001b[0m B, seq_len, n_embed \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    336\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(I)\n\u001b[1;32m--> 337\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues(x)\n\u001b[0;32m    340\u001b[0m weights \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(n_embed)  \u001b[38;5;66;03m# (B, Seq_len, Seq_len)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(bert.parameters(), lr=LEARNING_RATE)\n",
    "NUM_EPOCHS = 15 # epochs for pretrain task\n",
    "vacab_size = len(tokenizer.vocab)\n",
    "n_sp = len(tokenizer.special_tokens)\n",
    "for i in range(NUM_EPOCHS):\n",
    "    print(f'Epoch: {i + 1}')\n",
    "    train_loss = 0.0\n",
    "\n",
    "    bert.train()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('train'), total=dataloader.steps('train'), position=0)):\n",
    "        loss = 0\n",
    "        cudata = batch['input_ids'].to(device) # put the dat in the whole batch to gpu\n",
    "        # MLM part\n",
    "        for i in range(len(cudata)): # BATCH_SIZE = len(cudata)\n",
    "            mps, mcmt = mask_text(cudata[i], vacab_size, MAX_SEQ_LEN, n_sp)\n",
    "            lmps = len(mps)\n",
    "            _, r_mlm, _ = bert(mcmt.unsqueeze(dim=0)) # (Batch, Seq_Len, len(vocab))\n",
    "            MCMT = r_mlm[0][mps, :] # fectch the predicted value from masked input\n",
    "            predm = cudata[i][mps].long() # fectch the real word correspond to the masked input\n",
    "            for i in range(lmps):\n",
    "                loss += F.cross_entropy(MCMT.to(device), predm.to(device)) / lmps\n",
    "        \n",
    "        # NSP part\n",
    "        for i in range(len(cudata)): \n",
    "            spit, n_st = spit_cmt(cudata[i]) # (Batch, Seq_Len, len(vocab)), number of sentences\n",
    "            if n_st > 1: # calculate NSP loss if a comment has more than 1 sentence\n",
    "                predn = torch.tensor([1, 0] * (n_st - 1)).to(device) # label of next/not next composed sentences\n",
    "                if n_st < 3:\n",
    "                    predn = torch.tensor([1]).to(device)\n",
    "                _, _, r_nsp = bert(nsp_gen(spit, n_st, MAX_SEQ_LEN))\n",
    "#                 print(n_st)\n",
    "#                 print(r_nsp[:, 0, :].shape, predn.shape)\n",
    "#                 print(r_nsp[:, 0, :], predn)\n",
    "                loss +=  F.cross_entropy(r_nsp[:, 0, :], predn)\n",
    "#         print('train_loss:', '%.2f'%loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss += loss.item()\n",
    "    history_pretrain['train_losses'].append(train_loss / dataloader.steps(\"train\"))\n",
    "    print('train_loss:', '%.2f'%history_pretrain['train_losses'][-1])\n",
    "    val_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'para/'+ model + '_imdb_pre_para.pth'# parameter for pretrain task\n",
    "torch.save(bert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning on text classification downstream task, 20s/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Tokenization: 100%|██████████████████████████████████████████████████████| 20000/20000 [00:04<00:00, 4943.87it/s]\n",
      "Test Tokenization: 100%|███████████████████████████████████████████████████████| 25000/25000 [00:05<00:00, 4574.01it/s]\n",
      "Val Tokenization: 100%|██████████████████████████████████████████████████████████| 5000/5000 [00:02<00:00, 2341.60it/s]\n",
      "Train Label Encoding: 100%|██████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 833609.06it/s]\n",
      "Test Label Encoding: 100%|███████████████████████████████████████████████████| 25000/25000 [00:00<00:00, 940367.87it/s]\n",
      "Val Label Encoding: 100%|█████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 1376716.34it/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE_F = 32\n",
    "dataloader_F = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=BATCH_SIZE_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlLiYGbjcXPL",
    "outputId": "cfadd5cd-e236-4920-94ce-e3cffc3833db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████████████████▋                              | 388/625 [00:21<00:12, 19.03it/s]"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS_F = 200\n",
    "#optimizer = torch.optim.Adam(bert.parameters(), lr=LEARNING_RATE)\n",
    "for i in range(NUM_EPOCHS_F):\n",
    "    print(f'Epoch: {i + 1}')\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    bert.train()\n",
    "    for step, batch in enumerate(tqdm(dataloader_F.get_split('train'), total=dataloader_F.steps('train'), position=0)):\n",
    "        r_cls, _, _ = bert(batch['input_ids'].to(device)) # (B, Seq_Len, 2)\n",
    "\n",
    "        probs = F.softmax(r_cls[:, 0, :], dim=-1).cpu()# fetch the result from the first word in the text as output\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        train_preds += pred.detach().tolist()\n",
    "        train_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        loss = F.cross_entropy(r_cls[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    bert.eval()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('val'), total=dataloader.steps('val'), position=0)):\n",
    "        r_cls, _, _ = bert(batch['input_ids'].to(device))\n",
    "\n",
    "        probs = F.softmax(r_cls[:, 0, :], dim=-1).cpu()\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        val_preds += pred.detach().tolist()\n",
    "        val_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        loss = F.cross_entropy(r_cls[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    history['train_losses'].append(train_loss)\n",
    "    history['val_losses'].append(val_loss)\n",
    "    history['train_acc'].append(accuracy_score(train_labels, train_preds))\n",
    "    history['val_acc'].append(accuracy_score(val_labels, val_preds))\n",
    "    history['train_f1'].append(f1_score(train_labels, train_preds))\n",
    "    history['val_f1'].append(f1_score(val_labels, val_preds))\n",
    "\n",
    "    print()\n",
    "    print(f'Train loss: {train_loss / dataloader.steps(\"train\")} | Val loss: {val_loss / dataloader.steps(\"val\")}')\n",
    "    print(f'Train acc: {accuracy_score(train_labels, train_preds)} | Val acc: {accuracy_score(val_labels, val_preds)}')\n",
    "    print(f'Train f1: {f1_score(train_labels, train_preds)} | Val f1: {f1_score(val_labels, val_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'para/'+ model + 'imdb_tcl_para_BERT.pth'# parameter for text classification\n",
    "torch.save(bert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GWtJQPTtPRgf",
    "outputId": "624b70e4-b2dc-4878-f055-0f6754c819dd"
   },
   "outputs": [],
   "source": [
    "plot_results(history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FimwkDtWqzD",
    "outputId": "c2827b2a-9e07-4cc1-e198-940e0c1aa1d8"
   },
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "bert.eval()\n",
    "for step, batch in enumerate(tqdm(dataloader.get_split('test'), total=dataloader.steps('test'))):\n",
    "    logits = bert(batch['input_ids'].to(device))[0]\n",
    "\n",
    "    probs = F.softmax(logits[:, 0, :], dim=-1).cpu()\n",
    "    pred = torch.argmax(probs, dim=-1) # (B)\n",
    "    test_preds += pred.detach().tolist()\n",
    "    test_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "    loss = F.cross_entropy(logits[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "    test_loss += loss.item()\n",
    "\n",
    "print()\n",
    "print(f'Test loss: {test_loss / dataloader.steps(\"test\")}')\n",
    "print(f'Test acc: {accuracy_score(test_labels, test_preds)}')\n",
    "print(f'Test f1: {f1_score(test_labels, test_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlCBCY4xlUMj"
   },
   "source": [
    "# Interpreting and visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNon7i89dKoW"
   },
   "outputs": [],
   "source": [
    "def get_attention_scores(model, input_ids):\n",
    "    \"\"\"\n",
    "    This is just a wrapper to easily access attention heads of the last layer\n",
    "    \"\"\"\n",
    "\n",
    "    mask = (input_ids > 0).unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
    "\n",
    "    embed = model.nano_bert.embedding(input_ids)\n",
    "\n",
    "    # can be any layer, and we can also control what to do with output for each layer (aggregate, sum etc.)\n",
    "    layer = model.nano_bert.encoder.layers[-1]\n",
    "\n",
    "    x = layer.layer_norm1(embed)\n",
    "\n",
    "    B, seq_len, n_embed = x.shape\n",
    "\n",
    "    # if have more than 1 head, or interested in more than 1 head output just add aggregation here\n",
    "    head = layer.self_attention.heads[0]\n",
    "\n",
    "    # this is just a part of the single head that does all the computations (same code is present in AttentionHead)\n",
    "    q = head.query(x)\n",
    "    k = head.key(x)\n",
    "    v = head.values(x)\n",
    "\n",
    "    weights = (q @ k.transpose(-2, -1)) / math.sqrt(n_embed)  # (B, Seq_len, Seq_len)\n",
    "    weights = weights.masked_fill(mask == 0, -1e9)  # mask out not attended tokens\n",
    "\n",
    "    scores = F.softmax(weights, dim=-1)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cssoF5oSXC2V",
    "outputId": "7c10056c-56af-4db7-bd95-1d23bfceeb3e"
   },
   "outputs": [],
   "source": [
    "test_dataloader = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgyRO91Ng17v"
   },
   "outputs": [],
   "source": [
    "def plot_parallel(matrix, tokens):\n",
    "    # Set figsize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    input_len = len(tokens)\n",
    "\n",
    "    # Vertical lines\n",
    "    plt.axvline(x=1, color='black', linestyle='--', linewidth=1)\n",
    "    plt.axvline(x=5, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Add the A and B\n",
    "    plt.text(1, input_len + 1, 'A', fontsize=12, color='black', fontweight='bold')\n",
    "    plt.text(5, input_len + 1, 'B', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "    for i in range(input_len):\n",
    "        for j in range(input_len):\n",
    "            # Add the line to the plot\n",
    "            plt.plot([1, 5], [i, j], marker='o', label='token', color='blue', linewidth=5 * matrix[i][j])\n",
    "\n",
    "            plt.text(\n",
    "                1 - 0.18,  # x-axis position\n",
    "                i,  # y-axis position\n",
    "                tokens[i],  # Text\n",
    "                fontsize=8,  # Text size\n",
    "                color='black',  # Text color,\n",
    "            )\n",
    "\n",
    "            plt.text(\n",
    "                5 + 0.06,  # x-axis position\n",
    "                j,  # y-axis position\n",
    "                tokens[j],  # Text\n",
    "                fontsize=8,  # Text size\n",
    "                color='black',  # Text color\n",
    "            )\n",
    "        break\n",
    "\n",
    "    plt.title(f'Attention scores \\n\\n\\n')\n",
    "\n",
    "    plt.yticks([])  # Remove y-axis\n",
    "    plt.box(False)  # Remove the bounding box around plot\n",
    "    plt.show()  # Display the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRh1wc2Tf8Go",
    "outputId": "87c23765-e2cf-4b62-b741-553da6032f36"
   },
   "outputs": [],
   "source": [
    "# examples with less than 16 words are easier to visualize, so focus on them\n",
    "examples_ids = []\n",
    "for i, v in enumerate(test_dataloader.splits['test']):\n",
    "    if len(v) <= 16:\n",
    "        examples_ids.append(i)\n",
    "print(examples_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X2Mq57UVhD5F",
    "outputId": "a6da61db-cd06-4f91-a93b-c4ad625f6ec6"
   },
   "outputs": [],
   "source": [
    "for sample_index in examples_ids:\n",
    "    # extract example, decode to tokens and get the sequence length (ingoring padding)\n",
    "    test_tokenized_batch = test_dataloader.peek_index_tokenized(index=sample_index, split='test')\n",
    "    tokens = tokenizer.decode([t.item() for t in test_tokenized_batch['input_ids'][0] if (t != 0 and t.item() != 1)], ignore_special=False).split(' ')[:MAX_SEQ_LEN]\n",
    "    seq_len = len(tokens)\n",
    "\n",
    "    # calculate attention scores\n",
    "    att_matrix = get_attention_scores(bert, test_tokenized_batch['input_ids'].to(device))[0, :seq_len, :seq_len]\n",
    "\n",
    "    plot_parallel(att_matrix, tokens=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "311",
   "language": "python",
   "name": "311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
